{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    filters = 128  # 内层一维卷积核的数量，外层卷积核的数量应该等于embeddingSize，因为要确保每个layer后的输出维度和输入维度是一致的。\n",
    "    numHeads = 8  # Attention 的头数\n",
    "    numBlocks = 1  # 设置transformer block的数量\n",
    "    epsilon = 1e-8  # LayerNorm 层中的最小除数\n",
    "    keepProp = 0.9  # multi head attention 中的dropout\n",
    "    \n",
    "    dropoutKeepProb = 0.5 # 全连接层的dropout\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成位置嵌入\n",
    "def fixedPositionEmbedding(batchSize, sequenceLen):\n",
    "    embeddedPosition = []\n",
    "    for batch in range(batchSize):\n",
    "        x = []\n",
    "        for step in range(sequenceLen):\n",
    "            a = np.zeros(sequenceLen)\n",
    "            a[step] = 1\n",
    "            x.append(a)\n",
    "        embeddedPosition.append(x)\n",
    "    \n",
    "    return np.array(embeddedPosition, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.embeddedPosition = tf.placeholder(tf.float32, [None, config.sequenceLength, config.sequenceLength], name=\"embeddedPosition\")\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层, 位置向量的定义方式有两种：一是直接用固定的one-hot的形式传入，然后和词向量拼接，在当前的数据集上表现效果更好。另一种\n",
    "        # 就是按照论文中的方法实现，这样的效果反而更差，可能是增大了模型的复杂度，在小数据集上表现不佳。\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            self.embeddedWords = tf.concat([self.embedded, self.embeddedPosition], -1)\n",
    "\n",
    "        with tf.name_scope(\"transformer\"):\n",
    "            for i in range(config.model.numBlocks):\n",
    "                with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    multiHeadAtt = self._multiheadAttention(rawKeys=self.inputX, queries=self.embeddedWords,\n",
    "                                                            keys=self.embeddedWords)\n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    self.embeddedWords = self._feedForward(multiHeadAtt, \n",
    "                                                           [config.model.filters, config.model.embeddingSize + config.sequenceLength])\n",
    "                \n",
    "            outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize + config.sequenceLength)])\n",
    "\n",
    "        outputSize = outputs.get_shape()[-1].value\n",
    "\n",
    "#         with tf.name_scope(\"wordEmbedding\"):\n",
    "#             self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "#             self.wordEmbedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "        \n",
    "#         with tf.name_scope(\"positionEmbedding\"):\n",
    "#             print(self.wordEmbedded)\n",
    "#             self.positionEmbedded = self._positionEmbedding()\n",
    "            \n",
    "#         self.embeddedWords = self.wordEmbedded + self.positionEmbedded\n",
    "            \n",
    "#         with tf.name_scope(\"transformer\"):\n",
    "#             for i in range(config.model.numBlocks):\n",
    "#                 with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     multiHeadAtt = self._multiheadAttention(rawKeys=self.wordEmbedded, queries=self.embeddedWords,\n",
    "#                                                             keys=self.embeddedWords)\n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     self.embeddedWords = self._feedForward(multiHeadAtt, [config.model.filters, config.model.embeddingSize])\n",
    "                \n",
    "#             outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize)])\n",
    "\n",
    "#         outputSize = outputs.get_shape()[-1].value\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            outputs = tf.nn.dropout(outputs, keep_prob=self.dropoutKeepProb)\n",
    "    \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(outputs, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            \n",
    "    def _layerNormalization(self, inputs, scope=\"layerNorm\"):\n",
    "        # LayerNorm层和BN层有所不同\n",
    "        epsilon = self.config.model.epsilon\n",
    "\n",
    "        inputsShape = inputs.get_shape() # [batch_size, sequence_length, embedding_size]\n",
    "\n",
    "        paramsShape = inputsShape[-1:]\n",
    "\n",
    "        # LayerNorm是在最后的维度上计算输入的数据的均值和方差，BN层是考虑所有维度的\n",
    "        # mean, variance的维度都是[batch_size, sequence_len, 1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "\n",
    "        beta = tf.Variable(tf.zeros(paramsShape))\n",
    "\n",
    "        gamma = tf.Variable(tf.ones(paramsShape))\n",
    "        normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
    "        \n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "        return outputs\n",
    "            \n",
    "    def _multiheadAttention(self, rawKeys, queries, keys, numUnits=None, causality=False, scope=\"multiheadAttention\"):\n",
    "        # rawKeys 的作用是为了计算mask时用的，因为keys是加上了position embedding的，其中不存在padding为0的值\n",
    "        \n",
    "        numHeads = self.config.model.numHeads\n",
    "        keepProp = self.config.model.keepProp\n",
    "        \n",
    "        if numUnits is None:  # 若是没传入值，直接去输入数据的最后一维，即embedding size.\n",
    "            numUnits = queries.get_shape().as_list()[-1]\n",
    "\n",
    "        # tf.layers.dense可以做多维tensor数据的非线性映射，在计算self-Attention时，一定要对这三个值进行非线性映射，\n",
    "        # 其实这一步就是论文中Multi-Head Attention中的对分割后的数据进行权重映射的步骤，我们在这里先映射后分割，原则上是一样的。\n",
    "        # Q, K, V的维度都是[batch_size, sequence_length, embedding_size]\n",
    "        Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "        K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "        V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "\n",
    "        # 将数据按最后一维分割成num_heads个, 然后按照第一维拼接\n",
    "        # Q, K, V 的维度都是[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0) \n",
    "        K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0) \n",
    "        V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "\n",
    "        # 计算keys和queries之间的点积，维度[batch_size * numHeads, queries_len, key_len], 后两维是queries和keys的序列长度\n",
    "        similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "\n",
    "        # 对计算的点积进行缩放处理，除以向量长度的根号值\n",
    "        scaledSimilary = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "        # 在我们输入的序列中会存在padding这个样的填充词，这种词应该对最终的结果是毫无帮助的，原则上说当padding都是输入0时，\n",
    "        # 计算出来的权重应该也是0，但是在transformer中引入了位置向量，当和位置向量相加之后，其值就不为0了，因此在添加位置向量\n",
    "        # 之前，我们需要将其mask为0。虽然在queries中也存在这样的填充词，但原则上模型的结果之和输入有关，而且在self-Attention中\n",
    "        # queryies = keys，因此只要一方为0，计算出的权重就为0。\n",
    "        # 具体关于key mask的介绍可以看看这里： https://github.com/Kyubyong/transformer/issues/3\n",
    "\n",
    "        # 利用tf，tile进行张量扩张， 维度[batch_size * numHeads, keys_len] keys_len = keys 的序列长度\n",
    "        keyMasks = tf.tile(rawKeys, [numHeads, 1]) \n",
    "\n",
    "        # 增加一个维度，并进行扩张，得到维度[batch_size * numHeads, queries_len, keys_len]\n",
    "        keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "\n",
    "        # tf.ones_like生成元素全为1，维度和scaledSimilary相同的tensor, 然后得到负无穷大的值\n",
    "        paddings = tf.ones_like(scaledSimilary) * (-2 ** (32 + 1))\n",
    "\n",
    "        # tf.where(condition, x, y),condition中的元素为bool值，其中对应的True用x中的元素替换，对应的False用y中的元素替换\n",
    "        # 因此condition,x,y的维度是一样的。下面就是keyMasks中的值为0就用paddings中的值替换\n",
    "        maskedSimilary = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilary) # 维度[batch_size * numHeads, queries_len, key_len]\n",
    "\n",
    "        # 在计算当前的词时，只考虑上文，不考虑下文，出现在Transformer Decoder中。在文本分类时，可以只用Transformer Encoder。\n",
    "        # Decoder是生成模型，主要用在语言生成中\n",
    "        if causality:\n",
    "            diagVals = tf.ones_like(maskedSimilary[0, :, :])  # [queries_len, keys_len]\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diagVals).to_dense()  # [queries_len, keys_len]\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilary)[0], 1, 1])  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "            paddings = tf.ones_like(masks) * (-2 ** (32 + 1))\n",
    "            maskedSimilary = tf.where(tf.equal(masks, 0), paddings, maskedSimilary)  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "        # 通过softmax计算权重系数，维度 [batch_size * numHeads, queries_len, keys_len]\n",
    "        weights = tf.nn.softmax(maskedSimilary)\n",
    "\n",
    "        # 加权和得到输出值, 维度[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        outputs = tf.matmul(weights, V_)\n",
    "\n",
    "        # 将多头Attention计算的得到的输出重组成最初的维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "        \n",
    "        outputs = tf.nn.dropout(outputs, keep_prob=keepProp)\n",
    "\n",
    "        # 对每个subLayers建立残差连接，即H(x) = F(x) + x\n",
    "        outputs += queries\n",
    "        # normalization 层\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _feedForward(self, inputs, filters, scope=\"multiheadAttention\"):\n",
    "        # 在这里的前向传播采用卷积神经网络\n",
    "        \n",
    "        # 内层\n",
    "        params = {\"inputs\": inputs, \"filters\": filters[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 外层\n",
    "        params = {\"inputs\": outputs, \"filters\": filters[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "\n",
    "        # 这里用到了一维卷积，实际上卷积核尺寸还是二维的，只是只需要指定高度，宽度和embedding size的尺寸一致\n",
    "        # 维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 残差连接\n",
    "        outputs += inputs\n",
    "\n",
    "        # 归一化处理\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def _positionEmbedding(self, scope=\"positionEmbedding\"):\n",
    "        # 生成可训练的位置向量\n",
    "        batchSize = self.config.batchSize\n",
    "        sequenceLen = self.config.sequenceLength\n",
    "        embeddingSize = self.config.model.embeddingSize\n",
    "        \n",
    "        # 生成位置的索引，并扩张到batch中所有的样本上\n",
    "        positionIndex = tf.tile(tf.expand_dims(tf.range(sequenceLen), 0), [batchSize, 1])\n",
    "\n",
    "        # 根据正弦和余弦函数来获得每个位置上的embedding的第一部分\n",
    "        positionEmbedding = np.array([[pos / np.power(10000, (i-i%2) / embeddingSize) for i in range(embeddingSize)] \n",
    "                                      for pos in range(sequenceLen)])\n",
    "\n",
    "        # 然后根据奇偶性分别用sin和cos函数来包装\n",
    "        positionEmbedding[:, 0::2] = np.sin(positionEmbedding[:, 0::2])\n",
    "        positionEmbedding[:, 1::2] = np.cos(positionEmbedding[:, 1::2])\n",
    "\n",
    "        # 将positionEmbedding转换成tensor的格式\n",
    "        positionEmbedding_ = tf.cast(positionEmbedding, dtype=tf.float32)\n",
    "\n",
    "        # 得到三维的矩阵[batchSize, sequenceLen, embeddingSize]\n",
    "        positionEmbedded = tf.nn.embedding_lookup(positionEmbedding_, positionIndex)\n",
    "\n",
    "        return positionEmbedded\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/hist is illegal; using dense/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/sparsity is illegal; using dense/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/hist is illegal; using dense/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/sparsity is illegal; using dense/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/hist is illegal; using dense_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/sparsity is illegal; using dense_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/hist is illegal; using dense_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/sparsity is illegal; using dense_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/hist is illegal; using dense_2/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/sparsity is illegal; using dense_2/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/hist is illegal; using dense_2/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/sparsity is illegal; using dense_2/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/hist is illegal; using transformer/transformer-1/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/hist is illegal; using transformer/transformer-1/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/hist is illegal; using conv1d/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/sparsity is illegal; using conv1d/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/hist is illegal; using conv1d/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/sparsity is illegal; using conv1d/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/hist is illegal; using conv1d_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/sparsity is illegal; using conv1d_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/hist is illegal; using conv1d_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/sparsity is illegal; using conv1d_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/hist is illegal; using transformer/transformer-1/Variable_2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/hist is illegal; using transformer/transformer-1/Variable_3_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_3_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\githubProject\\textClassifier\\Transformer\\summarys\n",
      "\n",
      "start training model\n",
      "2019-05-21T14:20:19.715890, step: 1, loss: 0.9569775462150574, acc: 0.5, auc: 0.5125, precision: 0.5238, recall: 0.3333\n",
      "2019-05-21T14:20:19.955281, step: 2, loss: 12.783785820007324, acc: 0.4922, auc: 0.568, precision: 0.4922, recall: 1.0\n",
      "2019-05-21T14:20:20.147736, step: 3, loss: 4.347812175750732, acc: 0.4844, auc: 0.5352, precision: 0.0, recall: 0.0\n",
      "2019-05-21T14:20:20.344211, step: 4, loss: 1.9663095474243164, acc: 0.4844, auc: 0.4368, precision: 0.2727, recall: 0.1071\n",
      "2019-05-21T14:20:20.547666, step: 5, loss: 4.4606242179870605, acc: 0.4844, auc: 0.4873, precision: 0.4844, recall: 1.0\n",
      "2019-05-21T14:20:20.762094, step: 6, loss: 1.3185532093048096, acc: 0.4688, auc: 0.5242, precision: 0.4737, recall: 0.5625\n",
      "2019-05-21T14:20:20.958599, step: 7, loss: 4.239516258239746, acc: 0.4922, auc: 0.5705, precision: 0.0, recall: 0.0\n",
      "2019-05-21T14:20:21.151081, step: 8, loss: 2.7802212238311768, acc: 0.4922, auc: 0.5216, precision: 0.0, recall: 0.0\n",
      "2019-05-21T14:20:21.359526, step: 9, loss: 2.4973814487457275, acc: 0.4844, auc: 0.5425, precision: 0.4841, recall: 0.9839\n",
      "2019-05-21T14:20:21.572926, step: 10, loss: 3.3786678314208984, acc: 0.4297, auc: 0.59, precision: 0.4297, recall: 1.0\n",
      "2019-05-21T14:20:21.762418, step: 11, loss: 1.1425843238830566, acc: 0.5312, auc: 0.5348, precision: 0.5472, recall: 0.4462\n",
      "2019-05-21T14:20:21.959918, step: 12, loss: 2.3720486164093018, acc: 0.4922, auc: 0.6322, precision: 1.0, recall: 0.0152\n",
      "2019-05-21T14:20:22.157394, step: 13, loss: 1.8194494247436523, acc: 0.5234, auc: 0.6083, precision: 0.75, recall: 0.0476\n",
      "2019-05-21T14:20:22.348880, step: 14, loss: 1.3959780931472778, acc: 0.5469, auc: 0.5389, precision: 0.4648, recall: 0.6226\n",
      "2019-05-21T14:20:22.545354, step: 15, loss: 1.56386399269104, acc: 0.5625, auc: 0.5925, precision: 0.5165, recall: 0.7966\n",
      "2019-05-21T14:20:22.737810, step: 16, loss: 1.1493054628372192, acc: 0.5625, auc: 0.6166, precision: 0.5714, recall: 0.7536\n",
      "2019-05-21T14:20:22.935282, step: 17, loss: 1.4718492031097412, acc: 0.5234, auc: 0.4741, precision: 0.5116, recall: 0.3548\n",
      "2019-05-21T14:20:23.153726, step: 18, loss: 1.219981074333191, acc: 0.5859, auc: 0.6081, precision: 0.7143, recall: 0.3077\n",
      "2019-05-21T14:20:23.367127, step: 19, loss: 1.3207215070724487, acc: 0.4766, auc: 0.5351, precision: 0.5, recall: 0.2239\n",
      "2019-05-21T14:20:23.562604, step: 20, loss: 0.9959594011306763, acc: 0.5625, auc: 0.576, precision: 0.619, recall: 0.5493\n",
      "2019-05-21T14:20:23.757114, step: 21, loss: 0.9795234799385071, acc: 0.6172, auc: 0.6448, precision: 0.6186, recall: 0.8333\n",
      "2019-05-21T14:20:23.953559, step: 22, loss: 1.311570644378662, acc: 0.5156, auc: 0.5807, precision: 0.4839, recall: 0.7627\n",
      "2019-05-21T14:20:24.150065, step: 23, loss: 1.2716737985610962, acc: 0.4766, auc: 0.5038, precision: 0.5, recall: 0.2836\n",
      "2019-05-21T14:20:24.343516, step: 24, loss: 1.22803795337677, acc: 0.5312, auc: 0.5113, precision: 0.4, recall: 0.2222\n",
      "2019-05-21T14:20:24.536001, step: 25, loss: 1.270806908607483, acc: 0.5156, auc: 0.4813, precision: 0.52, recall: 0.2063\n",
      "2019-05-21T14:20:24.728515, step: 26, loss: 1.0936050415039062, acc: 0.5234, auc: 0.5703, precision: 0.5, recall: 0.6066\n",
      "2019-05-21T14:20:24.935961, step: 27, loss: 1.1337007284164429, acc: 0.5391, auc: 0.5659, precision: 0.4861, recall: 0.614\n",
      "2019-05-21T14:20:25.131439, step: 28, loss: 1.0422331094741821, acc: 0.5156, auc: 0.5525, precision: 0.561, recall: 0.3433\n",
      "2019-05-21T14:20:25.326915, step: 29, loss: 1.0498813390731812, acc: 0.5469, auc: 0.5629, precision: 0.5806, recall: 0.2857\n",
      "2019-05-21T14:20:25.521395, step: 30, loss: 1.1571241617202759, acc: 0.4844, auc: 0.5834, precision: 0.5714, recall: 0.2817\n",
      "2019-05-21T14:20:25.718867, step: 31, loss: 1.1604750156402588, acc: 0.5625, auc: 0.5583, precision: 0.5625, recall: 0.7941\n",
      "2019-05-21T14:20:25.908362, step: 32, loss: 1.3327434062957764, acc: 0.5, auc: 0.5028, precision: 0.4706, recall: 0.678\n",
      "2019-05-21T14:20:26.099850, step: 33, loss: 1.0612865686416626, acc: 0.5078, auc: 0.5414, precision: 0.5806, recall: 0.2647\n",
      "2019-05-21T14:20:26.291337, step: 34, loss: 1.1173202991485596, acc: 0.5625, auc: 0.6331, precision: 0.7586, recall: 0.3099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:20:26.485787, step: 35, loss: 0.7632007002830505, acc: 0.6406, auc: 0.6863, precision: 0.5926, recall: 0.5714\n",
      "2019-05-21T14:20:26.679270, step: 36, loss: 1.0213849544525146, acc: 0.5469, auc: 0.5941, precision: 0.5325, recall: 0.6508\n",
      "2019-05-21T14:20:26.867766, step: 37, loss: 0.8850215673446655, acc: 0.5938, auc: 0.6393, precision: 0.6296, recall: 0.5152\n",
      "2019-05-21T14:20:27.064269, step: 38, loss: 0.76563960313797, acc: 0.6562, auc: 0.6994, precision: 0.6364, recall: 0.5\n",
      "2019-05-21T14:20:27.260743, step: 39, loss: 1.0162913799285889, acc: 0.5312, auc: 0.6934, precision: 0.7241, recall: 0.2877\n",
      "2019-05-21T14:20:27.471153, step: 40, loss: 1.0002048015594482, acc: 0.6328, auc: 0.6562, precision: 0.6353, recall: 0.7714\n",
      "2019-05-21T14:20:27.663638, step: 41, loss: 0.9229214787483215, acc: 0.5938, auc: 0.6709, precision: 0.5698, recall: 0.7656\n",
      "2019-05-21T14:20:27.855125, step: 42, loss: 0.8876346349716187, acc: 0.625, auc: 0.6632, precision: 0.6667, recall: 0.5231\n",
      "2019-05-21T14:20:28.062572, step: 43, loss: 1.0221891403198242, acc: 0.5781, auc: 0.6318, precision: 0.6667, recall: 0.2295\n",
      "2019-05-21T14:20:28.260074, step: 44, loss: 0.9700413942337036, acc: 0.5469, auc: 0.6051, precision: 0.6053, recall: 0.3485\n",
      "2019-05-21T14:20:28.454552, step: 45, loss: 1.099739909172058, acc: 0.5938, auc: 0.6473, precision: 0.6019, recall: 0.8493\n",
      "2019-05-21T14:20:28.656012, step: 46, loss: 1.0308928489685059, acc: 0.5781, auc: 0.6519, precision: 0.5641, recall: 0.6875\n",
      "2019-05-21T14:20:28.853456, step: 47, loss: 1.0255686044692993, acc: 0.6484, auc: 0.7065, precision: 0.9394, recall: 0.4189\n",
      "2019-05-21T14:20:29.053950, step: 48, loss: 0.851476788520813, acc: 0.6719, auc: 0.6555, precision: 0.7209, recall: 0.5082\n",
      "2019-05-21T14:20:29.247403, step: 49, loss: 0.8636976480484009, acc: 0.6406, auc: 0.6838, precision: 0.6338, recall: 0.6923\n",
      "2019-05-21T14:20:29.438890, step: 50, loss: 0.6587357521057129, acc: 0.6797, auc: 0.7719, precision: 0.7101, recall: 0.7\n",
      "2019-05-21T14:20:29.652320, step: 51, loss: 0.7913233041763306, acc: 0.6562, auc: 0.7238, precision: 0.6667, recall: 0.5574\n",
      "2019-05-21T14:20:29.850789, step: 52, loss: 0.7357878684997559, acc: 0.6562, auc: 0.7634, precision: 0.775, recall: 0.4697\n",
      "2019-05-21T14:20:30.041309, step: 53, loss: 0.7234888672828674, acc: 0.7031, auc: 0.7809, precision: 0.7344, recall: 0.6912\n",
      "2019-05-21T14:20:30.233795, step: 54, loss: 0.656271755695343, acc: 0.7188, auc: 0.7893, precision: 0.6667, recall: 0.8197\n",
      "2019-05-21T14:20:30.425284, step: 55, loss: 0.6254784464836121, acc: 0.6953, auc: 0.8003, precision: 0.7451, recall: 0.5938\n",
      "2019-05-21T14:20:30.616741, step: 56, loss: 0.6102080345153809, acc: 0.7188, auc: 0.8492, precision: 0.8947, recall: 0.5152\n",
      "2019-05-21T14:20:30.808229, step: 57, loss: 0.6838028430938721, acc: 0.6953, auc: 0.7634, precision: 0.6769, recall: 0.7097\n",
      "2019-05-21T14:20:31.001711, step: 58, loss: 0.5456032752990723, acc: 0.7578, auc: 0.843, precision: 0.7742, recall: 0.7385\n",
      "2019-05-21T14:20:31.193229, step: 59, loss: 0.7050710916519165, acc: 0.7422, auc: 0.7845, precision: 0.7255, recall: 0.6607\n",
      "2019-05-21T14:20:31.396655, step: 60, loss: 0.8049734830856323, acc: 0.6406, auc: 0.7542, precision: 0.7381, recall: 0.4697\n",
      "2019-05-21T14:20:31.586181, step: 61, loss: 0.6133934855461121, acc: 0.7578, auc: 0.823, precision: 0.726, recall: 0.8281\n",
      "2019-05-21T14:20:31.777637, step: 62, loss: 0.7726038694381714, acc: 0.6797, auc: 0.7595, precision: 0.6909, recall: 0.6129\n",
      "2019-05-21T14:20:31.969124, step: 63, loss: 0.7298189401626587, acc: 0.6719, auc: 0.7548, precision: 0.6863, recall: 0.5738\n",
      "2019-05-21T14:20:32.160613, step: 64, loss: 0.7076361775398254, acc: 0.7109, auc: 0.8275, precision: 0.8718, recall: 0.5152\n",
      "2019-05-21T14:20:32.355121, step: 65, loss: 0.7210612297058105, acc: 0.7734, auc: 0.8089, precision: 0.7381, recall: 0.8986\n",
      "2019-05-21T14:20:32.564532, step: 66, loss: 0.5897222757339478, acc: 0.7188, auc: 0.8386, precision: 0.7018, recall: 0.678\n",
      "2019-05-21T14:20:32.759043, step: 67, loss: 0.7784504890441895, acc: 0.7188, auc: 0.8649, precision: 0.9459, recall: 0.5072\n",
      "2019-05-21T14:20:32.956513, step: 68, loss: 0.8184936046600342, acc: 0.7266, auc: 0.8108, precision: 0.662, recall: 0.8103\n",
      "2019-05-21T14:20:33.145005, step: 69, loss: 0.5295445919036865, acc: 0.7734, auc: 0.8706, precision: 0.7826, recall: 0.7941\n",
      "2019-05-21T14:20:33.336497, step: 70, loss: 0.6813549995422363, acc: 0.7891, auc: 0.8368, precision: 0.8448, recall: 0.7313\n",
      "2019-05-21T14:20:33.551921, step: 71, loss: 0.4157896935939789, acc: 0.8125, auc: 0.9027, precision: 0.875, recall: 0.7424\n",
      "2019-05-21T14:20:33.751383, step: 72, loss: 0.4636208117008209, acc: 0.7891, auc: 0.8944, precision: 0.8621, recall: 0.7246\n",
      "2019-05-21T14:20:33.943844, step: 73, loss: 0.8518850803375244, acc: 0.7578, auc: 0.826, precision: 0.7215, recall: 0.8636\n",
      "2019-05-21T14:20:34.135361, step: 74, loss: 0.6122325658798218, acc: 0.7344, auc: 0.8462, precision: 0.7358, recall: 0.661\n",
      "2019-05-21T14:20:34.351753, step: 75, loss: 0.6999226212501526, acc: 0.75, auc: 0.875, precision: 0.8824, recall: 0.6338\n",
      "2019-05-21T14:20:34.553244, step: 76, loss: 0.45831555128097534, acc: 0.8438, auc: 0.9021, precision: 0.8491, recall: 0.7895\n",
      "2019-05-21T14:20:34.751684, step: 77, loss: 0.5532647967338562, acc: 0.8281, auc: 0.9162, precision: 0.7875, recall: 0.9265\n",
      "2019-05-21T14:20:34.947161, step: 78, loss: 0.5988885164260864, acc: 0.7422, auc: 0.8606, precision: 0.806, recall: 0.7297\n",
      "2019-05-21T14:20:35.154606, step: 79, loss: 0.5706806778907776, acc: 0.7734, auc: 0.8904, precision: 0.9, recall: 0.6522\n",
      "2019-05-21T14:20:35.353075, step: 80, loss: 0.4938141703605652, acc: 0.8203, auc: 0.8892, precision: 0.8382, recall: 0.8261\n",
      "2019-05-21T14:20:35.549550, step: 81, loss: 0.5453670024871826, acc: 0.8281, auc: 0.9214, precision: 0.7465, recall: 0.9298\n",
      "2019-05-21T14:20:35.746053, step: 82, loss: 0.3115463852882385, acc: 0.8594, auc: 0.9482, precision: 0.9259, recall: 0.7812\n",
      "2019-05-21T14:20:35.939537, step: 83, loss: 0.5764317512512207, acc: 0.7656, auc: 0.8877, precision: 0.8723, recall: 0.6308\n",
      "2019-05-21T14:20:36.132012, step: 84, loss: 0.4658767580986023, acc: 0.8047, auc: 0.9042, precision: 0.7867, recall: 0.8676\n",
      "2019-05-21T14:20:36.348452, step: 85, loss: 0.5674266815185547, acc: 0.7891, auc: 0.8914, precision: 0.7595, recall: 0.8824\n",
      "2019-05-21T14:20:36.561876, step: 86, loss: 0.6714497804641724, acc: 0.8125, auc: 0.853, precision: 0.8889, recall: 0.6154\n",
      "2019-05-21T14:20:36.754328, step: 87, loss: 0.5159006714820862, acc: 0.8047, auc: 0.9082, precision: 0.86, recall: 0.7049\n",
      "2019-05-21T14:20:36.960776, step: 88, loss: 0.5795882940292358, acc: 0.8359, auc: 0.8775, precision: 0.791, recall: 0.8833\n",
      "2019-05-21T14:20:37.156254, step: 89, loss: 0.3413991332054138, acc: 0.8594, auc: 0.9366, precision: 0.8551, recall: 0.8806\n",
      "2019-05-21T14:20:37.350734, step: 90, loss: 0.655005931854248, acc: 0.7031, auc: 0.8666, precision: 0.8696, recall: 0.5556\n",
      "2019-05-21T14:20:37.547208, step: 91, loss: 0.47540992498397827, acc: 0.8594, auc: 0.9094, precision: 0.8485, recall: 0.875\n",
      "2019-05-21T14:20:37.741688, step: 92, loss: 0.5429753065109253, acc: 0.8125, auc: 0.8988, precision: 0.75, recall: 0.9\n",
      "2019-05-21T14:20:37.935171, step: 93, loss: 0.5577580332756042, acc: 0.75, auc: 0.8586, precision: 0.7458, recall: 0.7213\n",
      "2019-05-21T14:20:38.132643, step: 94, loss: 0.8090313673019409, acc: 0.7344, auc: 0.8887, precision: 0.931, recall: 0.4576\n",
      "2019-05-21T14:20:38.343111, step: 95, loss: 0.5198549032211304, acc: 0.7891, auc: 0.8904, precision: 0.8333, recall: 0.678\n",
      "2019-05-21T14:20:38.537561, step: 96, loss: 0.9295018911361694, acc: 0.75, auc: 0.8703, precision: 0.6706, recall: 0.9344\n",
      "2019-05-21T14:20:38.730075, step: 97, loss: 0.46043819189071655, acc: 0.8281, auc: 0.9037, precision: 0.8667, recall: 0.7879\n",
      "2019-05-21T14:20:38.922561, step: 98, loss: 0.6106979250907898, acc: 0.7969, auc: 0.868, precision: 0.8846, recall: 0.697\n",
      "2019-05-21T14:20:39.114018, step: 99, loss: 0.7067425847053528, acc: 0.7422, auc: 0.8255, precision: 0.7414, recall: 0.7049\n",
      "2019-05-21T14:20:39.311521, step: 100, loss: 0.5301392078399658, acc: 0.7734, auc: 0.8794, precision: 0.7358, recall: 0.7222\n",
      "\n",
      "Evaluation:\n",
      "2019-05-21T14:20:47.265251, step: 100, loss: 0.44544825645593494, acc: 0.8135025641025643, auc: 0.9154820512820516, precision: 0.8975153846153844, recall: 0.712053846153846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:20:47.456709, step: 101, loss: 0.49703481793403625, acc: 0.8125, auc: 0.9091, precision: 0.8913, recall: 0.6833\n",
      "2019-05-21T14:20:47.655179, step: 102, loss: 0.5653517246246338, acc: 0.8125, auc: 0.871, precision: 0.8814, recall: 0.7536\n",
      "2019-05-21T14:20:47.853648, step: 103, loss: 0.5030131340026855, acc: 0.8281, auc: 0.9048, precision: 0.8261, recall: 0.8507\n",
      "2019-05-21T14:20:48.046133, step: 104, loss: 0.3823525905609131, acc: 0.8359, auc: 0.929, precision: 0.8611, recall: 0.8493\n",
      "2019-05-21T14:20:48.238649, step: 105, loss: 0.5926997661590576, acc: 0.7891, auc: 0.8652, precision: 0.7903, recall: 0.7778\n",
      "2019-05-21T14:20:48.431104, step: 106, loss: 0.5325844883918762, acc: 0.7812, auc: 0.9312, precision: 0.9231, recall: 0.5902\n",
      "2019-05-21T14:20:48.624615, step: 107, loss: 0.494932621717453, acc: 0.7812, auc: 0.9023, precision: 0.7778, recall: 0.7778\n",
      "2019-05-21T14:20:48.814113, step: 108, loss: 0.4166436791419983, acc: 0.8359, auc: 0.9238, precision: 0.8281, recall: 0.8413\n",
      "2019-05-21T14:20:49.016538, step: 109, loss: 0.3436794579029083, acc: 0.8672, auc: 0.9345, precision: 0.8596, recall: 0.8448\n",
      "2019-05-21T14:20:49.227005, step: 110, loss: 0.5223198533058167, acc: 0.8203, auc: 0.8946, precision: 0.8936, recall: 0.7\n",
      "2019-05-21T14:20:49.420458, step: 111, loss: 0.5514611005783081, acc: 0.8203, auc: 0.886, precision: 0.8727, recall: 0.75\n",
      "2019-05-21T14:20:49.612973, step: 112, loss: 0.4297924339771271, acc: 0.8516, auc: 0.9249, precision: 0.8281, recall: 0.8689\n",
      "2019-05-21T14:20:49.814435, step: 113, loss: 0.572167158126831, acc: 0.7812, auc: 0.8995, precision: 0.7083, recall: 0.8793\n",
      "2019-05-21T14:20:50.024847, step: 114, loss: 0.8230172395706177, acc: 0.6875, auc: 0.9167, precision: 1.0, recall: 0.3846\n",
      "2019-05-21T14:20:50.217359, step: 115, loss: 0.6893800497055054, acc: 0.7578, auc: 0.8463, precision: 0.6716, recall: 0.8333\n",
      "2019-05-21T14:20:50.410839, step: 116, loss: 0.47135549783706665, acc: 0.7969, auc: 0.9034, precision: 0.7581, recall: 0.8103\n",
      "2019-05-21T14:20:50.620250, step: 117, loss: 0.5545735359191895, acc: 0.7891, auc: 0.8654, precision: 0.7674, recall: 0.66\n",
      "2019-05-21T14:20:50.813765, step: 118, loss: 0.26994234323501587, acc: 0.8594, auc: 0.9608, precision: 0.92, recall: 0.7667\n",
      "2019-05-21T14:20:51.026164, step: 119, loss: 0.4133148193359375, acc: 0.7891, auc: 0.9177, precision: 0.8947, recall: 0.7083\n",
      "2019-05-21T14:20:51.217653, step: 120, loss: 0.5681594610214233, acc: 0.8203, auc: 0.9111, precision: 0.7662, recall: 0.9219\n",
      "2019-05-21T14:20:51.436068, step: 121, loss: 0.40019750595092773, acc: 0.8516, auc: 0.935, precision: 0.8421, recall: 0.9014\n",
      "2019-05-21T14:20:51.634568, step: 122, loss: 0.31132790446281433, acc: 0.8672, auc: 0.9467, precision: 0.9245, recall: 0.7903\n",
      "2019-05-21T14:20:51.827056, step: 123, loss: 0.5975004434585571, acc: 0.7734, auc: 0.8904, precision: 0.8966, recall: 0.5\n",
      "2019-05-21T14:20:52.020534, step: 124, loss: 0.5805659294128418, acc: 0.7578, auc: 0.8711, precision: 0.8364, recall: 0.6765\n",
      "2019-05-21T14:20:52.214018, step: 125, loss: 0.9032365083694458, acc: 0.7734, auc: 0.8674, precision: 0.7, recall: 0.918\n",
      "2019-05-21T14:20:52.406473, step: 126, loss: 0.4217432141304016, acc: 0.8438, auc: 0.9298, precision: 0.8312, recall: 0.9014\n",
      "2019-05-21T14:20:52.596994, step: 127, loss: 0.4932461082935333, acc: 0.8203, auc: 0.9143, precision: 0.8889, recall: 0.6897\n",
      "2019-05-21T14:20:52.814384, step: 128, loss: 0.6215630769729614, acc: 0.7656, auc: 0.9555, precision: 1.0, recall: 0.5522\n",
      "2019-05-21T14:20:53.006897, step: 129, loss: 0.45188871026039124, acc: 0.8828, auc: 0.9331, precision: 0.8533, recall: 0.9412\n",
      "2019-05-21T14:20:53.197387, step: 130, loss: 0.7947324514389038, acc: 0.7734, auc: 0.925, precision: 0.6966, recall: 0.9688\n",
      "2019-05-21T14:20:53.403806, step: 131, loss: 0.3412024676799774, acc: 0.8672, auc: 0.9431, precision: 0.9444, recall: 0.7846\n",
      "2019-05-21T14:20:53.597318, step: 132, loss: 0.5231512188911438, acc: 0.7812, auc: 0.9163, precision: 0.8649, recall: 0.5818\n",
      "2019-05-21T14:20:53.789804, step: 133, loss: 0.5846061706542969, acc: 0.7891, auc: 0.9152, precision: 0.9412, recall: 0.6667\n",
      "2019-05-21T14:20:53.987246, step: 134, loss: 0.5225784182548523, acc: 0.8125, auc: 0.9126, precision: 0.7429, recall: 0.8966\n",
      "2019-05-21T14:20:54.186713, step: 135, loss: 0.4746578335762024, acc: 0.8438, auc: 0.9458, precision: 0.8125, recall: 0.9286\n",
      "2019-05-21T14:20:54.383216, step: 136, loss: 0.4891894459724426, acc: 0.8828, auc: 0.9284, precision: 0.8806, recall: 0.8939\n",
      "2019-05-21T14:20:54.597642, step: 137, loss: 0.6335033178329468, acc: 0.7422, auc: 0.9104, precision: 0.9778, recall: 0.5789\n",
      "2019-05-21T14:20:54.788134, step: 138, loss: 0.45334529876708984, acc: 0.8203, auc: 0.9302, precision: 0.8868, recall: 0.7344\n",
      "2019-05-21T14:20:54.980619, step: 139, loss: 0.5869873762130737, acc: 0.8047, auc: 0.9352, precision: 0.7143, recall: 0.9483\n",
      "2019-05-21T14:20:55.189061, step: 140, loss: 0.40583691000938416, acc: 0.8125, auc: 0.9256, precision: 0.8525, recall: 0.7761\n",
      "2019-05-21T14:20:55.382545, step: 141, loss: 0.3646918833255768, acc: 0.8828, auc: 0.9469, precision: 0.9242, recall: 0.8592\n",
      "2019-05-21T14:20:55.587997, step: 142, loss: 0.444166898727417, acc: 0.8594, auc: 0.9343, precision: 0.9077, recall: 0.831\n",
      "2019-05-21T14:20:55.790425, step: 143, loss: 0.7369734048843384, acc: 0.7969, auc: 0.8632, precision: 0.7671, recall: 0.8615\n",
      "2019-05-21T14:20:55.982940, step: 144, loss: 0.4402293562889099, acc: 0.8438, auc: 0.9271, precision: 0.8727, recall: 0.7869\n",
      "2019-05-21T14:20:56.189358, step: 145, loss: 0.2921159267425537, acc: 0.8516, auc: 0.9563, precision: 0.9344, recall: 0.7917\n",
      "2019-05-21T14:20:56.384865, step: 146, loss: 0.37577080726623535, acc: 0.8516, auc: 0.9404, precision: 0.8305, recall: 0.8448\n",
      "2019-05-21T14:20:56.577320, step: 147, loss: 0.2780896723270416, acc: 0.875, auc: 0.9597, precision: 0.9123, recall: 0.8254\n",
      "2019-05-21T14:20:56.769834, step: 148, loss: 0.3224698305130005, acc: 0.8906, auc: 0.9472, precision: 0.8571, recall: 0.8889\n",
      "2019-05-21T14:20:56.993208, step: 149, loss: 0.5592098832130432, acc: 0.8203, auc: 0.9046, precision: 0.8667, recall: 0.7761\n",
      "2019-05-21T14:20:57.185722, step: 150, loss: 0.5149608254432678, acc: 0.8203, auc: 0.8957, precision: 0.8542, recall: 0.7193\n",
      "2019-05-21T14:20:57.377181, step: 151, loss: 0.4313048720359802, acc: 0.8281, auc: 0.923, precision: 0.7755, recall: 0.7755\n",
      "2019-05-21T14:20:57.569697, step: 152, loss: 0.49092113971710205, acc: 0.8594, auc: 0.9118, precision: 0.8939, recall: 0.8429\n",
      "2019-05-21T14:20:57.780133, step: 153, loss: 0.531319797039032, acc: 0.8281, auc: 0.8972, precision: 0.8281, recall: 0.8281\n",
      "2019-05-21T14:20:57.988577, step: 154, loss: 0.41569608449935913, acc: 0.8359, auc: 0.9229, precision: 0.8276, recall: 0.8136\n",
      "2019-05-21T14:20:58.180067, step: 155, loss: 0.6975648403167725, acc: 0.7578, auc: 0.8625, precision: 0.8136, recall: 0.7059\n",
      "2019-05-21T14:20:58.380526, step: 156, loss: 0.4345299005508423, acc: 0.8438, auc: 0.9259, precision: 0.9048, recall: 0.8028\n",
      "start training model\n",
      "2019-05-21T14:20:58.588974, step: 157, loss: 0.3871386647224426, acc: 0.8359, auc: 0.9259, precision: 0.8125, recall: 0.8525\n",
      "2019-05-21T14:20:58.783421, step: 158, loss: 0.34179598093032837, acc: 0.8516, auc: 0.9346, precision: 0.871, recall: 0.8308\n",
      "2019-05-21T14:20:59.006823, step: 159, loss: 0.338386595249176, acc: 0.8516, auc: 0.9407, precision: 0.8966, recall: 0.8\n",
      "2019-05-21T14:20:59.198341, step: 160, loss: 0.3001170754432678, acc: 0.875, auc: 0.9472, precision: 0.8627, recall: 0.8302\n",
      "2019-05-21T14:20:59.394818, step: 161, loss: 0.33887720108032227, acc: 0.8984, auc: 0.9511, precision: 0.9355, recall: 0.8657\n",
      "2019-05-21T14:20:59.607218, step: 162, loss: 0.2797023355960846, acc: 0.8594, auc: 0.958, precision: 0.9273, recall: 0.7846\n",
      "2019-05-21T14:20:59.814663, step: 163, loss: 0.3232938349246979, acc: 0.9062, auc: 0.9564, precision: 0.8636, recall: 0.95\n",
      "2019-05-21T14:21:00.008181, step: 164, loss: 0.22852462530136108, acc: 0.9219, auc: 0.97, precision: 0.9298, recall: 0.8983\n",
      "2019-05-21T14:21:00.200661, step: 165, loss: 0.26371490955352783, acc: 0.8594, auc: 0.9586, precision: 0.9032, recall: 0.8235\n",
      "2019-05-21T14:21:00.402092, step: 166, loss: 0.3547315001487732, acc: 0.875, auc: 0.9438, precision: 0.9273, recall: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:21:00.613556, step: 167, loss: 0.23260141909122467, acc: 0.9062, auc: 0.9677, precision: 0.88, recall: 0.88\n",
      "2019-05-21T14:21:00.826986, step: 168, loss: 0.32750940322875977, acc: 0.8828, auc: 0.9521, precision: 0.8636, recall: 0.9048\n",
      "2019-05-21T14:21:01.032408, step: 169, loss: 0.20751681923866272, acc: 0.9453, auc: 0.9742, precision: 0.9298, recall: 0.9464\n",
      "2019-05-21T14:21:01.240880, step: 170, loss: 0.31353941559791565, acc: 0.8438, auc: 0.9878, precision: 1.0, recall: 0.7222\n",
      "2019-05-21T14:21:01.433335, step: 171, loss: 0.24505707621574402, acc: 0.8828, auc: 0.9657, precision: 0.8814, recall: 0.8667\n",
      "2019-05-21T14:21:01.622858, step: 172, loss: 0.43998441100120544, acc: 0.8438, auc: 0.9648, precision: 0.7722, recall: 0.9683\n",
      "2019-05-21T14:21:01.818306, step: 173, loss: 0.31682246923446655, acc: 0.8594, auc: 0.9496, precision: 0.8451, recall: 0.8955\n",
      "2019-05-21T14:21:02.010790, step: 174, loss: 0.5361911058425903, acc: 0.8125, auc: 0.9174, precision: 0.9167, recall: 0.6111\n",
      "2019-05-21T14:21:02.202278, step: 175, loss: 0.8568822145462036, acc: 0.75, auc: 0.8935, precision: 0.9722, recall: 0.5303\n",
      "2019-05-21T14:21:02.407757, step: 176, loss: 0.37513086199760437, acc: 0.8672, auc: 0.957, precision: 0.8169, recall: 0.9355\n",
      "2019-05-21T14:21:02.601217, step: 177, loss: 0.7317739725112915, acc: 0.8047, auc: 0.9661, precision: 0.7396, recall: 1.0\n",
      "2019-05-21T14:21:02.792730, step: 178, loss: 0.2452164739370346, acc: 0.8906, auc: 0.9644, precision: 0.9091, recall: 0.8475\n",
      "2019-05-21T14:21:02.995188, step: 179, loss: 0.3471962809562683, acc: 0.8828, auc: 0.9744, precision: 1.0, recall: 0.7414\n",
      "2019-05-21T14:21:03.198657, step: 180, loss: 0.7240270376205444, acc: 0.7812, auc: 0.9621, precision: 1.0, recall: 0.5821\n",
      "2019-05-21T14:21:03.411072, step: 181, loss: 0.13794419169425964, acc: 0.9453, auc: 0.9895, precision: 0.9265, recall: 0.9692\n",
      "2019-05-21T14:21:03.621514, step: 182, loss: 0.6456284523010254, acc: 0.8047, auc: 0.9465, precision: 0.7284, recall: 0.9516\n",
      "2019-05-21T14:21:03.816989, step: 183, loss: 0.3479039669036865, acc: 0.8906, auc: 0.9573, precision: 0.88, recall: 0.9296\n",
      "2019-05-21T14:21:04.020445, step: 184, loss: 0.3278016448020935, acc: 0.8984, auc: 0.9548, precision: 0.918, recall: 0.875\n",
      "2019-05-21T14:21:04.218915, step: 185, loss: 0.45661163330078125, acc: 0.8516, auc: 0.9546, precision: 0.9808, recall: 0.7391\n",
      "2019-05-21T14:21:04.416387, step: 186, loss: 0.5816595554351807, acc: 0.8281, auc: 0.9357, precision: 0.9643, recall: 0.7297\n",
      "2019-05-21T14:21:04.610838, step: 187, loss: 0.25629734992980957, acc: 0.8984, auc: 0.9726, precision: 0.875, recall: 0.9589\n",
      "2019-05-21T14:21:04.808341, step: 188, loss: 0.5833103656768799, acc: 0.8516, auc: 0.9433, precision: 0.8052, recall: 0.9394\n",
      "2019-05-21T14:21:05.014758, step: 189, loss: 0.462906152009964, acc: 0.8594, auc: 0.9508, precision: 0.7945, recall: 0.9508\n",
      "2019-05-21T14:21:05.219211, step: 190, loss: 0.41524970531463623, acc: 0.8516, auc: 0.925, precision: 0.8594, recall: 0.8462\n",
      "2019-05-21T14:21:05.409702, step: 191, loss: 0.4794318377971649, acc: 0.8203, auc: 0.9776, precision: 1.0, recall: 0.6714\n",
      "2019-05-21T14:21:05.605208, step: 192, loss: 0.21660426259040833, acc: 0.9297, auc: 0.9851, precision: 1.0, recall: 0.8657\n",
      "2019-05-21T14:21:05.797693, step: 193, loss: 0.39845019578933716, acc: 0.875, auc: 0.9419, precision: 0.8548, recall: 0.8833\n",
      "2019-05-21T14:21:06.008131, step: 194, loss: 0.33737853169441223, acc: 0.875, auc: 0.9616, precision: 0.8485, recall: 0.9032\n",
      "2019-05-21T14:21:06.206597, step: 195, loss: 0.3144037127494812, acc: 0.8984, auc: 0.9683, precision: 0.8676, recall: 0.9365\n",
      "2019-05-21T14:21:06.398088, step: 196, loss: 0.21827448904514313, acc: 0.9141, auc: 0.9869, precision: 1.0, recall: 0.8429\n",
      "2019-05-21T14:21:06.587583, step: 197, loss: 0.22356680035591125, acc: 0.9219, auc: 0.9753, precision: 0.9524, recall: 0.8955\n",
      "2019-05-21T14:21:06.781035, step: 198, loss: 0.23484480381011963, acc: 0.8984, auc: 0.9724, precision: 0.9444, recall: 0.8361\n",
      "2019-05-21T14:21:06.975545, step: 199, loss: 0.33004230260849, acc: 0.8984, auc: 0.9595, precision: 0.9322, recall: 0.8594\n",
      "2019-05-21T14:21:07.165039, step: 200, loss: 0.4035964608192444, acc: 0.875, auc: 0.9424, precision: 0.8361, recall: 0.8947\n",
      "\n",
      "Evaluation:\n",
      "2019-05-21T14:21:14.798671, step: 200, loss: 0.47305775223634183, acc: 0.8499641025641028, auc: 0.9322358974358972, precision: 0.8165102564102563, recall: 0.9068487179487178\n",
      "2019-05-21T14:21:14.989160, step: 201, loss: 0.3563152551651001, acc: 0.9062, auc: 0.9526, precision: 0.8732, recall: 0.9538\n",
      "2019-05-21T14:21:15.183645, step: 202, loss: 0.2924951910972595, acc: 0.9062, auc: 0.9617, precision: 0.8806, recall: 0.9365\n",
      "2019-05-21T14:21:15.377094, step: 203, loss: 0.2742335796356201, acc: 0.8672, auc: 0.9613, precision: 0.9556, recall: 0.7414\n",
      "2019-05-21T14:21:15.595509, step: 204, loss: 0.4432156980037689, acc: 0.7734, auc: 0.9477, precision: 0.95, recall: 0.5846\n",
      "2019-05-21T14:21:15.815920, step: 205, loss: 0.384914755821228, acc: 0.8984, auc: 0.9408, precision: 0.9, recall: 0.8852\n",
      "2019-05-21T14:21:16.009433, step: 206, loss: 0.34464162588119507, acc: 0.8906, auc: 0.9496, precision: 0.88, recall: 0.9296\n",
      "2019-05-21T14:21:16.202915, step: 207, loss: 0.281467080116272, acc: 0.8984, auc: 0.9626, precision: 0.8904, recall: 0.9286\n",
      "2019-05-21T14:21:16.396402, step: 208, loss: 0.16594535112380981, acc: 0.9219, auc: 0.9824, precision: 0.9351, recall: 0.9351\n",
      "2019-05-21T14:21:16.618773, step: 209, loss: 0.3898661732673645, acc: 0.8672, auc: 0.9272, precision: 0.8571, recall: 0.766\n",
      "2019-05-21T14:21:16.833200, step: 210, loss: 0.49597975611686707, acc: 0.8359, auc: 0.932, precision: 0.9286, recall: 0.7536\n",
      "2019-05-21T14:21:17.046629, step: 211, loss: 0.28367671370506287, acc: 0.9062, auc: 0.9626, precision: 0.9206, recall: 0.8923\n",
      "2019-05-21T14:21:17.240112, step: 212, loss: 0.32673051953315735, acc: 0.875, auc: 0.9525, precision: 0.9333, recall: 0.8235\n",
      "2019-05-21T14:21:17.430602, step: 213, loss: 0.7224563360214233, acc: 0.7969, auc: 0.9084, precision: 0.725, recall: 0.9355\n",
      "2019-05-21T14:21:17.627107, step: 214, loss: 0.42017489671707153, acc: 0.875, auc: 0.9392, precision: 0.8305, recall: 0.8909\n",
      "2019-05-21T14:21:17.820590, step: 215, loss: 0.49744415283203125, acc: 0.8203, auc: 0.9542, precision: 0.9636, recall: 0.7162\n",
      "2019-05-21T14:21:18.016037, step: 216, loss: 0.34219077229499817, acc: 0.8672, auc: 0.9701, precision: 1.0, recall: 0.6731\n",
      "2019-05-21T14:21:18.230494, step: 217, loss: 0.3614978492259979, acc: 0.8984, auc: 0.9482, precision: 0.9298, recall: 0.8548\n",
      "2019-05-21T14:21:18.444890, step: 218, loss: 0.5098131895065308, acc: 0.8594, auc: 0.9577, precision: 0.7895, recall: 0.9677\n",
      "2019-05-21T14:21:18.654361, step: 219, loss: 0.32713162899017334, acc: 0.9219, auc: 0.9553, precision: 0.9, recall: 0.9545\n",
      "2019-05-21T14:21:18.846846, step: 220, loss: 0.23137503862380981, acc: 0.9062, auc: 0.9718, precision: 0.8871, recall: 0.9167\n",
      "2019-05-21T14:21:19.052296, step: 221, loss: 0.33053404092788696, acc: 0.8281, auc: 0.9739, precision: 0.9184, recall: 0.7143\n",
      "2019-05-21T14:21:19.245749, step: 222, loss: 0.470279335975647, acc: 0.8125, auc: 0.9553, precision: 0.9459, recall: 0.614\n",
      "2019-05-21T14:21:19.455223, step: 223, loss: 0.30858150124549866, acc: 0.8906, auc: 0.9633, precision: 0.8615, recall: 0.918\n",
      "2019-05-21T14:21:19.647674, step: 224, loss: 0.590410590171814, acc: 0.7969, auc: 0.9345, precision: 0.725, recall: 0.9355\n",
      "2019-05-21T14:21:19.844177, step: 225, loss: 0.42723697423934937, acc: 0.8438, auc: 0.9422, precision: 0.7937, recall: 0.8772\n",
      "2019-05-21T14:21:20.037631, step: 226, loss: 0.5827952027320862, acc: 0.8203, auc: 0.9467, precision: 1.0, recall: 0.5818\n",
      "2019-05-21T14:21:20.229150, step: 227, loss: 0.40837714076042175, acc: 0.8438, auc: 0.965, precision: 0.9444, recall: 0.75\n",
      "2019-05-21T14:21:20.425627, step: 228, loss: 0.23800590634346008, acc: 0.9219, auc: 0.9724, precision: 0.9, recall: 0.9545\n",
      "2019-05-21T14:21:20.618112, step: 229, loss: 0.3547731339931488, acc: 0.8906, auc: 0.9555, precision: 0.8701, recall: 0.9437\n",
      "2019-05-21T14:21:20.814553, step: 230, loss: 0.5310325622558594, acc: 0.875, auc: 0.9432, precision: 0.84, recall: 0.9403\n",
      "2019-05-21T14:21:21.007069, step: 231, loss: 0.3054268956184387, acc: 0.8672, auc: 0.9607, precision: 0.9286, recall: 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:21:21.208530, step: 232, loss: 0.34142979979515076, acc: 0.8828, auc: 0.9462, precision: 0.9286, recall: 0.8667\n",
      "2019-05-21T14:21:21.402980, step: 233, loss: 0.1815945953130722, acc: 0.9297, auc: 0.9826, precision: 0.9322, recall: 0.9167\n",
      "2019-05-21T14:21:21.611423, step: 234, loss: 0.34454718232154846, acc: 0.8672, auc: 0.9432, precision: 0.8679, recall: 0.8214\n",
      "2019-05-21T14:21:21.820863, step: 235, loss: 0.5183038115501404, acc: 0.7734, auc: 0.9036, precision: 0.8182, recall: 0.7031\n",
      "2019-05-21T14:21:22.032329, step: 236, loss: 0.3657265305519104, acc: 0.8828, auc: 0.9409, precision: 0.8814, recall: 0.8667\n",
      "2019-05-21T14:21:22.229799, step: 237, loss: 0.22168314456939697, acc: 0.9141, auc: 0.9744, precision: 0.9589, recall: 0.8974\n",
      "2019-05-21T14:21:22.423252, step: 238, loss: 0.3593842387199402, acc: 0.9219, auc: 0.9533, precision: 0.8889, recall: 0.9697\n",
      "2019-05-21T14:21:22.637679, step: 239, loss: 0.3857070207595825, acc: 0.8906, auc: 0.9446, precision: 0.8667, recall: 0.8966\n",
      "2019-05-21T14:21:22.836177, step: 240, loss: 0.3689451813697815, acc: 0.8672, auc: 0.9477, precision: 0.9231, recall: 0.8333\n",
      "2019-05-21T14:21:23.033620, step: 241, loss: 0.2734193801879883, acc: 0.875, auc: 0.9571, precision: 0.8846, recall: 0.8214\n",
      "2019-05-21T14:21:23.232089, step: 242, loss: 0.3721667528152466, acc: 0.875, auc: 0.9432, precision: 0.8361, recall: 0.8947\n",
      "2019-05-21T14:21:23.426569, step: 243, loss: 0.3115854859352112, acc: 0.875, auc: 0.9555, precision: 0.9259, recall: 0.8065\n",
      "2019-05-21T14:21:23.640995, step: 244, loss: 0.2490343600511551, acc: 0.9141, auc: 0.9711, precision: 0.9688, recall: 0.8732\n",
      "2019-05-21T14:21:23.837499, step: 245, loss: 0.2979116141796112, acc: 0.8828, auc: 0.959, precision: 0.9118, recall: 0.8732\n",
      "2019-05-21T14:21:24.031982, step: 246, loss: 0.26457273960113525, acc: 0.9297, auc: 0.9672, precision: 0.918, recall: 0.9333\n",
      "2019-05-21T14:21:24.227458, step: 247, loss: 0.2573443055152893, acc: 0.9062, auc: 0.9706, precision: 0.8947, recall: 0.8947\n",
      "2019-05-21T14:21:24.426894, step: 248, loss: 0.28130680322647095, acc: 0.9219, auc: 0.9663, precision: 0.9118, recall: 0.9394\n",
      "2019-05-21T14:21:24.632345, step: 249, loss: 0.19039401412010193, acc: 0.8984, auc: 0.9791, precision: 0.8621, recall: 0.9091\n",
      "2019-05-21T14:21:24.837795, step: 250, loss: 0.25897666811943054, acc: 0.9141, auc: 0.9731, precision: 0.9643, recall: 0.8571\n",
      "2019-05-21T14:21:25.032304, step: 251, loss: 0.39911943674087524, acc: 0.8516, auc: 0.9565, precision: 0.9583, recall: 0.7302\n",
      "2019-05-21T14:21:25.225790, step: 252, loss: 0.27807968854904175, acc: 0.9062, auc: 0.9626, precision: 0.9322, recall: 0.873\n",
      "2019-05-21T14:21:25.435199, step: 253, loss: 0.34974205493927, acc: 0.8672, auc: 0.9566, precision: 0.8308, recall: 0.9\n",
      "2019-05-21T14:21:25.645636, step: 254, loss: 0.2891208231449127, acc: 0.9062, auc: 0.9729, precision: 0.8553, recall: 0.9848\n",
      "2019-05-21T14:21:25.839146, step: 255, loss: 0.3430844843387604, acc: 0.9141, auc: 0.9415, precision: 0.9, recall: 0.9153\n",
      "2019-05-21T14:21:26.032630, step: 256, loss: 0.26808518171310425, acc: 0.8594, auc: 0.9773, precision: 0.9783, recall: 0.7258\n",
      "2019-05-21T14:21:26.237084, step: 257, loss: 0.190020352602005, acc: 0.9062, auc: 0.9845, precision: 0.9844, recall: 0.8514\n",
      "2019-05-21T14:21:26.432560, step: 258, loss: 0.23994295299053192, acc: 0.9141, auc: 0.9675, precision: 0.931, recall: 0.8852\n",
      "2019-05-21T14:21:26.637981, step: 259, loss: 0.40624868869781494, acc: 0.8672, auc: 0.933, precision: 0.8548, recall: 0.8689\n",
      "2019-05-21T14:21:26.831493, step: 260, loss: 0.2947441339492798, acc: 0.9375, auc: 0.9751, precision: 0.9014, recall: 0.9846\n",
      "2019-05-21T14:21:27.033952, step: 261, loss: 0.28844037652015686, acc: 0.8906, auc: 0.96, precision: 0.9508, recall: 0.8406\n",
      "2019-05-21T14:21:27.226438, step: 262, loss: 0.3261207342147827, acc: 0.8828, auc: 0.9575, precision: 0.931, recall: 0.8308\n",
      "2019-05-21T14:21:27.419932, step: 263, loss: 0.3271241784095764, acc: 0.8438, auc: 0.9469, precision: 0.9231, recall: 0.6792\n",
      "2019-05-21T14:21:27.628361, step: 264, loss: 0.2865012288093567, acc: 0.8828, auc: 0.9678, precision: 0.9615, recall: 0.7937\n",
      "2019-05-21T14:21:27.823835, step: 265, loss: 0.38768622279167175, acc: 0.8516, auc: 0.9389, precision: 0.8382, recall: 0.8769\n",
      "2019-05-21T14:21:28.022279, step: 266, loss: 0.34892594814300537, acc: 0.8906, auc: 0.968, precision: 0.7931, recall: 0.9583\n",
      "2019-05-21T14:21:28.218782, step: 267, loss: 0.3385546803474426, acc: 0.8281, auc: 0.9455, precision: 0.9216, recall: 0.7231\n",
      "2019-05-21T14:21:28.420215, step: 268, loss: 0.4198344051837921, acc: 0.8047, auc: 0.9328, precision: 0.878, recall: 0.6429\n",
      "2019-05-21T14:21:28.626692, step: 269, loss: 0.2589424252510071, acc: 0.8828, auc: 0.9764, precision: 0.9825, recall: 0.8\n",
      "2019-05-21T14:21:28.821143, step: 270, loss: 0.5307389497756958, acc: 0.8438, auc: 0.9527, precision: 0.7302, recall: 0.9388\n",
      "2019-05-21T14:21:29.012631, step: 271, loss: 0.22055113315582275, acc: 0.9297, auc: 0.975, precision: 0.9403, recall: 0.9265\n",
      "2019-05-21T14:21:29.206142, step: 272, loss: 0.3372628092765808, acc: 0.8594, auc: 0.9362, precision: 0.8704, recall: 0.8103\n",
      "2019-05-21T14:21:29.426524, step: 273, loss: 0.27872514724731445, acc: 0.9297, auc: 0.9653, precision: 0.9701, recall: 0.9028\n",
      "2019-05-21T14:21:29.620035, step: 274, loss: 0.39198726415634155, acc: 0.8594, auc: 0.947, precision: 0.9286, recall: 0.7879\n",
      "2019-05-21T14:21:29.811496, step: 275, loss: 0.255479633808136, acc: 0.9141, auc: 0.9698, precision: 0.98, recall: 0.8305\n",
      "2019-05-21T14:21:30.023957, step: 276, loss: 0.43592119216918945, acc: 0.8516, auc: 0.9306, precision: 0.8475, recall: 0.8333\n",
      "2019-05-21T14:21:30.218407, step: 277, loss: 0.2172253429889679, acc: 0.9297, auc: 0.9733, precision: 0.9231, recall: 0.96\n",
      "2019-05-21T14:21:30.410892, step: 278, loss: 0.294746071100235, acc: 0.9141, auc: 0.9763, precision: 0.8611, recall: 0.9841\n",
      "2019-05-21T14:21:30.603377, step: 279, loss: 0.43976640701293945, acc: 0.8281, auc: 0.9593, precision: 0.963, recall: 0.7222\n",
      "2019-05-21T14:21:30.811820, step: 280, loss: 0.4776734709739685, acc: 0.8359, auc: 0.9309, precision: 0.8929, recall: 0.7692\n",
      "2019-05-21T14:21:31.005303, step: 281, loss: 0.3165258765220642, acc: 0.875, auc: 0.9536, precision: 0.8852, recall: 0.8571\n",
      "2019-05-21T14:21:31.207764, step: 282, loss: 0.3642772138118744, acc: 0.8828, auc: 0.9534, precision: 0.8537, recall: 0.9589\n",
      "2019-05-21T14:21:31.406230, step: 283, loss: 0.4816510081291199, acc: 0.8281, auc: 0.9297, precision: 0.7917, recall: 0.8906\n",
      "2019-05-21T14:21:31.598745, step: 284, loss: 0.46597495675086975, acc: 0.8516, auc: 0.9417, precision: 0.9362, recall: 0.7333\n",
      "2019-05-21T14:21:31.809183, step: 285, loss: 0.39811354875564575, acc: 0.8203, auc: 0.9736, precision: 0.9722, recall: 0.614\n",
      "2019-05-21T14:21:32.006625, step: 286, loss: 0.28591135144233704, acc: 0.8359, auc: 0.95, precision: 0.8545, recall: 0.7833\n",
      "2019-05-21T14:21:32.199140, step: 287, loss: 0.40108761191368103, acc: 0.8672, auc: 0.9503, precision: 0.8143, recall: 0.9344\n",
      "2019-05-21T14:21:32.391626, step: 288, loss: 0.3296924829483032, acc: 0.8828, auc: 0.9608, precision: 0.8841, recall: 0.8971\n",
      "2019-05-21T14:21:32.609014, step: 289, loss: 0.34909719228744507, acc: 0.8828, auc: 0.9439, precision: 0.9014, recall: 0.8889\n",
      "2019-05-21T14:21:32.806487, step: 290, loss: 0.3169545829296112, acc: 0.8672, auc: 0.9615, precision: 0.9254, recall: 0.8378\n",
      "2019-05-21T14:21:33.001995, step: 291, loss: 0.3345543146133423, acc: 0.875, auc: 0.953, precision: 0.8276, recall: 0.8889\n",
      "2019-05-21T14:21:33.198468, step: 292, loss: 0.48583632707595825, acc: 0.8281, auc: 0.9228, precision: 0.8966, recall: 0.7647\n",
      "2019-05-21T14:21:33.414891, step: 293, loss: 0.5012910962104797, acc: 0.8359, auc: 0.9219, precision: 0.9344, recall: 0.7703\n",
      "2019-05-21T14:21:33.618315, step: 294, loss: 0.3368341624736786, acc: 0.8672, auc: 0.9537, precision: 0.8625, recall: 0.92\n",
      "2019-05-21T14:21:33.812826, step: 295, loss: 0.566218912601471, acc: 0.8203, auc: 0.9468, precision: 0.7467, recall: 0.9333\n",
      "2019-05-21T14:21:34.029247, step: 296, loss: 0.282964825630188, acc: 0.8906, auc: 0.957, precision: 0.9412, recall: 0.8649\n",
      "2019-05-21T14:21:34.218710, step: 297, loss: 0.37651339173316956, acc: 0.8359, auc: 0.974, precision: 0.9828, recall: 0.7403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:21:34.412221, step: 298, loss: 0.3138146996498108, acc: 0.8516, auc: 0.9479, precision: 0.8983, recall: 0.803\n",
      "2019-05-21T14:21:34.604678, step: 299, loss: 0.3419109880924225, acc: 0.8672, auc: 0.9473, precision: 0.8644, recall: 0.85\n",
      "2019-05-21T14:21:34.797193, step: 300, loss: 0.2530079782009125, acc: 0.9062, auc: 0.965, precision: 0.9028, recall: 0.9286\n",
      "\n",
      "Evaluation:\n",
      "2019-05-21T14:21:42.412829, step: 300, loss: 0.45153632989296544, acc: 0.8541692307692309, auc: 0.9386820512820514, precision: 0.8102999999999999, recall: 0.9271256410256411\n",
      "2019-05-21T14:21:42.601325, step: 301, loss: 0.3496413230895996, acc: 0.8672, auc: 0.9665, precision: 0.8154, recall: 0.9138\n",
      "2019-05-21T14:21:42.795774, step: 302, loss: 0.3013482093811035, acc: 0.8516, auc: 0.956, precision: 0.8182, recall: 0.8852\n",
      "2019-05-21T14:21:42.995269, step: 303, loss: 0.36115628480911255, acc: 0.8516, auc: 0.9615, precision: 0.9767, recall: 0.7\n",
      "2019-05-21T14:21:43.192713, step: 304, loss: 0.5099265575408936, acc: 0.8125, auc: 0.9545, precision: 0.9778, recall: 0.6567\n",
      "2019-05-21T14:21:43.398163, step: 305, loss: 0.3821919858455658, acc: 0.8828, auc: 0.9451, precision: 0.8689, recall: 0.8833\n",
      "2019-05-21T14:21:43.603614, step: 306, loss: 0.21486087143421173, acc: 0.9297, auc: 0.9853, precision: 0.9062, recall: 0.9508\n",
      "2019-05-21T14:21:43.819038, step: 307, loss: 0.3817169666290283, acc: 0.8594, auc: 0.9535, precision: 0.8308, recall: 0.8852\n",
      "2019-05-21T14:21:44.013517, step: 308, loss: 0.20640401542186737, acc: 0.9219, auc: 0.9799, precision: 0.9028, recall: 0.9559\n",
      "2019-05-21T14:21:44.210990, step: 309, loss: 0.31776535511016846, acc: 0.8672, auc: 0.963, precision: 0.9636, recall: 0.7794\n",
      "2019-05-21T14:21:44.410485, step: 310, loss: 0.42248594760894775, acc: 0.8516, auc: 0.9402, precision: 0.9455, recall: 0.7647\n",
      "2019-05-21T14:21:44.604966, step: 311, loss: 0.4359455704689026, acc: 0.8672, auc: 0.9407, precision: 0.8852, recall: 0.8438\n",
      "2019-05-21T14:21:44.799416, step: 312, loss: 0.2819540500640869, acc: 0.8906, auc: 0.9761, precision: 0.8493, recall: 0.9538\n",
      "start training model\n",
      "2019-05-21T14:21:45.005864, step: 313, loss: 0.16602647304534912, acc: 0.9531, auc: 0.9918, precision: 0.9153, recall: 0.9818\n",
      "2019-05-21T14:21:45.204365, step: 314, loss: 0.1232854425907135, acc: 0.9688, auc: 0.989, precision: 0.9841, recall: 0.9538\n",
      "2019-05-21T14:21:45.402803, step: 315, loss: 0.14506438374519348, acc: 0.9375, auc: 0.9924, precision: 0.9818, recall: 0.8852\n",
      "2019-05-21T14:21:45.597311, step: 316, loss: 0.21273910999298096, acc: 0.8906, auc: 0.985, precision: 0.96, recall: 0.8\n",
      "2019-05-21T14:21:45.790796, step: 317, loss: 0.10545589029788971, acc: 0.9531, auc: 0.9938, precision: 0.9459, recall: 0.9722\n",
      "2019-05-21T14:21:45.987269, step: 318, loss: 0.09960401803255081, acc: 0.9609, auc: 0.9983, precision: 0.9219, recall: 1.0\n",
      "2019-05-21T14:21:46.179754, step: 319, loss: 0.2768406867980957, acc: 0.9219, auc: 0.9619, precision: 0.9242, recall: 0.9242\n",
      "2019-05-21T14:21:46.375232, step: 320, loss: 0.21437110006809235, acc: 0.9297, auc: 0.9755, precision: 0.9577, recall: 0.9189\n",
      "2019-05-21T14:21:46.565693, step: 321, loss: 0.1527373194694519, acc: 0.9453, auc: 0.9884, precision: 0.9701, recall: 0.9286\n",
      "2019-05-21T14:21:46.758210, step: 322, loss: 0.06355952471494675, acc: 0.9688, auc: 0.998, precision: 1.0, recall: 0.942\n",
      "2019-05-21T14:21:46.953656, step: 323, loss: 0.2343256175518036, acc: 0.9062, auc: 0.9755, precision: 0.9231, recall: 0.8955\n",
      "2019-05-21T14:21:47.147138, step: 324, loss: 0.1363999992609024, acc: 0.9688, auc: 0.99, precision: 0.9688, recall: 0.9688\n",
      "2019-05-21T14:21:47.344638, step: 325, loss: 0.1061239242553711, acc: 0.9531, auc: 0.9934, precision: 0.9667, recall: 0.9355\n",
      "2019-05-21T14:21:47.538123, step: 326, loss: 0.05140245705842972, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9853\n",
      "2019-05-21T14:21:47.736562, step: 327, loss: 0.09183070808649063, acc: 0.9766, auc: 0.9941, precision: 0.9792, recall: 0.9592\n",
      "2019-05-21T14:21:47.930044, step: 328, loss: 0.12734150886535645, acc: 0.9375, auc: 0.9948, precision: 0.9848, recall: 0.9028\n",
      "2019-05-21T14:21:48.123556, step: 329, loss: 0.10466393083333969, acc: 0.9375, auc: 0.9965, precision: 1.0, recall: 0.8889\n",
      "2019-05-21T14:21:48.320030, step: 330, loss: 0.11615379899740219, acc: 0.9609, auc: 0.9932, precision: 0.9538, recall: 0.9688\n",
      "2019-05-21T14:21:48.520497, step: 331, loss: 0.2759373188018799, acc: 0.9141, auc: 0.9798, precision: 0.8507, recall: 0.9828\n",
      "2019-05-21T14:21:48.723951, step: 332, loss: 0.1578691303730011, acc: 0.9453, auc: 0.9864, precision: 0.9577, recall: 0.9444\n",
      "2019-05-21T14:21:48.917435, step: 333, loss: 0.23112094402313232, acc: 0.9219, auc: 0.9756, precision: 0.9661, recall: 0.8769\n",
      "2019-05-21T14:21:49.115903, step: 334, loss: 0.37037041783332825, acc: 0.8828, auc: 0.978, precision: 1.0, recall: 0.7887\n",
      "2019-05-21T14:21:49.308359, step: 335, loss: 0.17593839764595032, acc: 0.9375, auc: 0.9848, precision: 0.9275, recall: 0.9552\n",
      "2019-05-21T14:21:49.501870, step: 336, loss: 0.04598003625869751, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-05-21T14:21:49.694326, step: 337, loss: 0.1767641007900238, acc: 0.9453, auc: 0.9926, precision: 0.9189, recall: 0.9855\n",
      "2019-05-21T14:21:49.892796, step: 338, loss: 0.28553617000579834, acc: 0.9219, auc: 0.9807, precision: 0.8923, recall: 0.9508\n",
      "2019-05-21T14:21:50.089301, step: 339, loss: 0.2123037725687027, acc: 0.9375, auc: 0.9809, precision: 0.9062, recall: 0.9667\n",
      "2019-05-21T14:21:50.300705, step: 340, loss: 0.161948099732399, acc: 0.9141, auc: 0.9929, precision: 1.0, recall: 0.8281\n",
      "2019-05-21T14:21:50.512141, step: 341, loss: 0.3762308359146118, acc: 0.8281, auc: 0.9856, precision: 0.9778, recall: 0.6769\n",
      "2019-05-21T14:21:50.704625, step: 342, loss: 0.2016219049692154, acc: 0.9375, auc: 0.9875, precision: 0.9836, recall: 0.8955\n",
      "2019-05-21T14:21:50.904092, step: 343, loss: 0.14260098338127136, acc: 0.9609, auc: 0.9939, precision: 0.9254, recall: 1.0\n",
      "2019-05-21T14:21:51.105553, step: 344, loss: 0.3513248860836029, acc: 0.875, auc: 0.9939, precision: 0.7867, recall: 1.0\n",
      "2019-05-21T14:21:51.299067, step: 345, loss: 0.11445573717355728, acc: 0.9609, auc: 0.9933, precision: 0.9583, recall: 0.9718\n",
      "2019-05-21T14:21:51.492518, step: 346, loss: 0.2294466495513916, acc: 0.9062, auc: 0.9887, precision: 0.9828, recall: 0.8382\n",
      "2019-05-21T14:21:51.685003, step: 347, loss: 0.18463453650474548, acc: 0.9141, auc: 0.986, precision: 0.9833, recall: 0.8551\n",
      "2019-05-21T14:21:51.881478, step: 348, loss: 0.23277774453163147, acc: 0.9297, auc: 0.9697, precision: 0.9242, recall: 0.9385\n",
      "2019-05-21T14:21:52.079947, step: 349, loss: 0.09853299707174301, acc: 0.9609, auc: 0.9944, precision: 0.9683, recall: 0.9531\n",
      "2019-05-21T14:21:52.271464, step: 350, loss: 0.21326841413974762, acc: 0.9375, auc: 0.9824, precision: 0.9275, recall: 0.9552\n",
      "2019-05-21T14:21:52.464918, step: 351, loss: 0.12802617251873016, acc: 0.9531, auc: 0.9922, precision: 0.9355, recall: 0.9667\n",
      "2019-05-21T14:21:52.662390, step: 352, loss: 0.17274831235408783, acc: 0.9141, auc: 0.9863, precision: 0.9524, recall: 0.8824\n",
      "2019-05-21T14:21:52.856870, step: 353, loss: 0.16362081468105316, acc: 0.9297, auc: 0.9917, precision: 1.0, recall: 0.8525\n",
      "2019-05-21T14:21:53.051379, step: 354, loss: 0.19357526302337646, acc: 0.9062, auc: 0.9763, precision: 0.9219, recall: 0.8939\n",
      "2019-05-21T14:21:53.249847, step: 355, loss: 0.07908801734447479, acc: 0.9688, auc: 0.9966, precision: 0.9706, recall: 0.9706\n",
      "2019-05-21T14:21:53.442333, step: 356, loss: 0.06720839440822601, acc: 0.9688, auc: 0.999, precision: 0.9559, recall: 0.9848\n",
      "2019-05-21T14:21:53.635820, step: 357, loss: 0.10702825337648392, acc: 0.9688, auc: 0.9951, precision: 0.9545, recall: 0.9844\n",
      "2019-05-21T14:21:53.833289, step: 358, loss: 0.11550520360469818, acc: 0.9375, auc: 0.9929, precision: 0.9688, recall: 0.9118\n",
      "2019-05-21T14:21:54.029733, step: 359, loss: 0.11511480808258057, acc: 0.9688, auc: 0.9921, precision: 0.9853, recall: 0.9571\n",
      "2019-05-21T14:21:54.226239, step: 360, loss: 0.1423724889755249, acc: 0.9453, auc: 0.9872, precision: 0.9643, recall: 0.9153\n",
      "2019-05-21T14:21:54.418724, step: 361, loss: 0.23508884012699127, acc: 0.9062, auc: 0.98, precision: 0.8714, recall: 0.9531\n",
      "2019-05-21T14:21:54.609213, step: 362, loss: 0.15117144584655762, acc: 0.9531, auc: 0.9893, precision: 0.9455, recall: 0.9455\n",
      "2019-05-21T14:21:54.809678, step: 363, loss: 0.22541341185569763, acc: 0.9062, auc: 0.9827, precision: 0.9649, recall: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:21:55.002164, step: 364, loss: 0.22309333086013794, acc: 0.9531, auc: 0.982, precision: 0.9851, recall: 0.9296\n",
      "2019-05-21T14:21:55.204592, step: 365, loss: 0.08549501746892929, acc: 0.9609, auc: 0.9944, precision: 0.9559, recall: 0.9701\n",
      "2019-05-21T14:21:55.405086, step: 366, loss: 0.1551872044801712, acc: 0.9375, auc: 0.9895, precision: 0.9123, recall: 0.9455\n",
      "2019-05-21T14:21:55.607544, step: 367, loss: 0.1141849234700203, acc: 0.9609, auc: 0.9926, precision: 0.9706, recall: 0.9565\n",
      "2019-05-21T14:21:55.817951, step: 368, loss: 0.11624563485383987, acc: 0.9453, auc: 0.9931, precision: 0.942, recall: 0.9559\n",
      "2019-05-21T14:21:56.015423, step: 369, loss: 0.27165913581848145, acc: 0.9297, auc: 0.9721, precision: 0.9524, recall: 0.9091\n",
      "2019-05-21T14:21:56.210931, step: 370, loss: 0.10924561321735382, acc: 0.9531, auc: 0.9934, precision: 0.95, recall: 0.95\n",
      "2019-05-21T14:21:56.409370, step: 371, loss: 0.10485701262950897, acc: 0.9375, auc: 0.9933, precision: 0.9608, recall: 0.8909\n",
      "2019-05-21T14:21:56.602882, step: 372, loss: 0.11565632373094559, acc: 0.9531, auc: 0.9923, precision: 0.971, recall: 0.9437\n",
      "2019-05-21T14:21:56.795371, step: 373, loss: 0.09416195005178452, acc: 0.9453, auc: 0.9975, precision: 1.0, recall: 0.875\n",
      "2019-05-21T14:21:56.998823, step: 374, loss: 0.06319622695446014, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-05-21T14:21:57.191311, step: 375, loss: 0.2353876531124115, acc: 0.8906, auc: 0.9822, precision: 0.8333, recall: 0.9259\n",
      "2019-05-21T14:21:57.383764, step: 376, loss: 0.11671976745128632, acc: 0.9375, auc: 0.9934, precision: 0.9322, recall: 0.9322\n",
      "2019-05-21T14:21:57.582265, step: 377, loss: 0.16901351511478424, acc: 0.9297, auc: 0.9897, precision: 0.9831, recall: 0.8788\n",
      "2019-05-21T14:21:57.777740, step: 378, loss: 0.13626879453659058, acc: 0.9453, auc: 0.9901, precision: 0.9848, recall: 0.9155\n",
      "2019-05-21T14:21:57.971222, step: 379, loss: 0.1503111720085144, acc: 0.9453, auc: 0.9907, precision: 0.9836, recall: 0.9091\n",
      "2019-05-21T14:21:58.161685, step: 380, loss: 0.11777670681476593, acc: 0.9688, auc: 0.9926, precision: 0.9655, recall: 0.9655\n",
      "2019-05-21T14:21:58.354198, step: 381, loss: 0.15114261209964752, acc: 0.9688, auc: 0.9954, precision: 0.942, recall: 1.0\n",
      "2019-05-21T14:21:58.563610, step: 382, loss: 0.08128311485052109, acc: 0.9609, auc: 0.9961, precision: 0.9524, recall: 0.9677\n",
      "2019-05-21T14:21:58.753133, step: 383, loss: 0.12397463619709015, acc: 0.9375, auc: 0.9936, precision: 0.9839, recall: 0.8971\n",
      "2019-05-21T14:21:58.951572, step: 384, loss: 0.10933036357164383, acc: 0.9688, auc: 0.9902, precision: 0.9848, recall: 0.9559\n",
      "2019-05-21T14:21:59.145054, step: 385, loss: 0.11357656121253967, acc: 0.9766, auc: 0.9934, precision: 0.9672, recall: 0.9833\n",
      "2019-05-21T14:21:59.350535, step: 386, loss: 0.1392105668783188, acc: 0.9531, auc: 0.9916, precision: 0.931, recall: 0.9643\n",
      "2019-05-21T14:21:59.545982, step: 387, loss: 0.10282551497220993, acc: 0.9766, auc: 0.9949, precision: 1.0, recall: 0.9552\n",
      "2019-05-21T14:21:59.739465, step: 388, loss: 0.16099843382835388, acc: 0.9375, auc: 0.991, precision: 0.9828, recall: 0.8906\n",
      "2019-05-21T14:21:59.938967, step: 389, loss: 0.10132630169391632, acc: 0.9766, auc: 0.9956, precision: 1.0, recall: 0.9571\n",
      "2019-05-21T14:22:00.138432, step: 390, loss: 0.11838392913341522, acc: 0.9531, auc: 0.991, precision: 0.9722, recall: 0.9459\n",
      "2019-05-21T14:22:00.335900, step: 391, loss: 0.12195004522800446, acc: 0.9688, auc: 0.9923, precision: 0.9718, recall: 0.9718\n",
      "2019-05-21T14:22:00.534339, step: 392, loss: 0.1882619857788086, acc: 0.9531, auc: 0.9908, precision: 0.9153, recall: 0.9818\n",
      "2019-05-21T14:22:00.741785, step: 393, loss: 0.17725461721420288, acc: 0.9375, auc: 0.9885, precision: 0.9818, recall: 0.8852\n",
      "2019-05-21T14:22:00.936296, step: 394, loss: 0.23872143030166626, acc: 0.9062, auc: 0.9778, precision: 0.9683, recall: 0.8592\n",
      "2019-05-21T14:22:01.136729, step: 395, loss: 0.0921022817492485, acc: 0.9297, auc: 0.994, precision: 0.9592, recall: 0.8704\n",
      "2019-05-21T14:22:01.332225, step: 396, loss: 0.15169775485992432, acc: 0.9453, auc: 0.9895, precision: 0.9275, recall: 0.9697\n",
      "2019-05-21T14:22:01.530675, step: 397, loss: 0.08283975720405579, acc: 0.9688, auc: 0.9958, precision: 0.9833, recall: 0.9516\n",
      "2019-05-21T14:22:01.728182, step: 398, loss: 0.15613631904125214, acc: 0.9609, auc: 0.9866, precision: 0.9524, recall: 0.9677\n",
      "2019-05-21T14:22:01.924622, step: 399, loss: 0.1745685636997223, acc: 0.9219, auc: 0.9854, precision: 0.9655, recall: 0.875\n",
      "2019-05-21T14:22:02.118104, step: 400, loss: 0.0959794670343399, acc: 0.9531, auc: 0.9948, precision: 0.9701, recall: 0.942\n",
      "\n",
      "Evaluation:\n",
      "2019-05-21T14:22:10.201489, step: 400, loss: 0.4543466969178273, acc: 0.8711948717948715, auc: 0.9423051282051285, precision: 0.8700666666666669, recall: 0.8751384615384615\n",
      "2019-05-21T14:22:10.396966, step: 401, loss: 0.1363040804862976, acc: 0.9375, auc: 0.9905, precision: 0.9365, recall: 0.9365\n",
      "2019-05-21T14:22:10.597430, step: 402, loss: 0.10469088703393936, acc: 0.9766, auc: 0.9914, precision: 0.971, recall: 0.9853\n",
      "2019-05-21T14:22:10.801883, step: 403, loss: 0.059465304017066956, acc: 0.9766, auc: 0.998, precision: 0.9811, recall: 0.963\n",
      "2019-05-21T14:22:11.002347, step: 404, loss: 0.11517995595932007, acc: 0.9688, auc: 0.9932, precision: 0.9841, recall: 0.9538\n",
      "2019-05-21T14:22:11.202811, step: 405, loss: 0.12373340874910355, acc: 0.9609, auc: 0.9925, precision: 0.9286, recall: 0.9811\n",
      "2019-05-21T14:22:11.398288, step: 406, loss: 0.15694911777973175, acc: 0.9453, auc: 0.9876, precision: 0.9804, recall: 0.8929\n",
      "2019-05-21T14:22:11.596758, step: 407, loss: 0.2663365304470062, acc: 0.9062, auc: 0.9709, precision: 0.9062, recall: 0.9062\n",
      "2019-05-21T14:22:11.796253, step: 408, loss: 0.16547495126724243, acc: 0.9297, auc: 0.9904, precision: 0.98, recall: 0.8596\n",
      "2019-05-21T14:22:11.991732, step: 409, loss: 0.17404593527317047, acc: 0.9375, auc: 0.9846, precision: 0.9605, recall: 0.9359\n",
      "2019-05-21T14:22:12.190171, step: 410, loss: 0.26874426007270813, acc: 0.8984, auc: 0.9817, precision: 0.8254, recall: 0.963\n",
      "2019-05-21T14:22:12.389638, step: 411, loss: 0.24645361304283142, acc: 0.9375, auc: 0.9741, precision: 0.9219, recall: 0.9516\n",
      "2019-05-21T14:22:12.586140, step: 412, loss: 0.15089482069015503, acc: 0.9141, auc: 0.988, precision: 0.9322, recall: 0.8871\n",
      "2019-05-21T14:22:12.782587, step: 413, loss: 0.119267039000988, acc: 0.9453, auc: 0.9948, precision: 1.0, recall: 0.9\n",
      "2019-05-21T14:22:12.986074, step: 414, loss: 0.10293493419885635, acc: 0.9688, auc: 0.9948, precision: 0.971, recall: 0.971\n",
      "2019-05-21T14:22:13.179553, step: 415, loss: 0.11555065959692001, acc: 0.9531, auc: 0.9927, precision: 0.9833, recall: 0.9219\n",
      "2019-05-21T14:22:13.379023, step: 416, loss: 0.13567394018173218, acc: 0.9609, auc: 0.9919, precision: 0.942, recall: 0.9848\n",
      "2019-05-21T14:22:13.572504, step: 417, loss: 0.18697398900985718, acc: 0.9141, auc: 0.9807, precision: 0.9104, recall: 0.9242\n",
      "2019-05-21T14:22:13.767983, step: 418, loss: 0.16201168298721313, acc: 0.9375, auc: 0.9929, precision: 1.0, recall: 0.8806\n",
      "2019-05-21T14:22:13.974400, step: 419, loss: 0.13847756385803223, acc: 0.9453, auc: 0.989, precision: 0.9841, recall: 0.9118\n",
      "2019-05-21T14:22:14.174896, step: 420, loss: 0.15155990421772003, acc: 0.9297, auc: 0.9902, precision: 0.8939, recall: 0.9672\n",
      "2019-05-21T14:22:14.401469, step: 421, loss: 0.16320960223674774, acc: 0.9141, auc: 0.9888, precision: 0.9322, recall: 0.8871\n",
      "2019-05-21T14:22:14.595920, step: 422, loss: 0.15494856238365173, acc: 0.9375, auc: 0.9883, precision: 0.9206, recall: 0.9508\n",
      "2019-05-21T14:22:14.786430, step: 423, loss: 0.08192958682775497, acc: 0.9688, auc: 0.9963, precision: 0.9831, recall: 0.9508\n",
      "2019-05-21T14:22:14.979894, step: 424, loss: 0.12155792117118835, acc: 0.9453, auc: 0.9949, precision: 1.0, recall: 0.8923\n",
      "2019-05-21T14:22:15.174402, step: 425, loss: 0.2534525692462921, acc: 0.9297, auc: 0.9714, precision: 0.9508, recall: 0.9062\n",
      "2019-05-21T14:22:15.374837, step: 426, loss: 0.20406755805015564, acc: 0.8984, auc: 0.9792, precision: 0.8806, recall: 0.9219\n",
      "2019-05-21T14:22:15.573339, step: 427, loss: 0.12955255806446075, acc: 0.9453, auc: 0.9897, precision: 0.9516, recall: 0.9365\n",
      "2019-05-21T14:22:15.771804, step: 428, loss: 0.12505324184894562, acc: 0.9609, auc: 0.9911, precision: 0.9649, recall: 0.9483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:22:15.970245, step: 429, loss: 0.12483979016542435, acc: 0.9609, auc: 0.9922, precision: 1.0, recall: 0.9265\n",
      "2019-05-21T14:22:16.166720, step: 430, loss: 0.1907961070537567, acc: 0.9219, auc: 0.9788, precision: 0.9286, recall: 0.9286\n",
      "2019-05-21T14:22:16.362240, step: 431, loss: 0.1423654705286026, acc: 0.9219, auc: 0.9914, precision: 0.9012, recall: 0.9733\n",
      "2019-05-21T14:22:16.559701, step: 432, loss: 0.12445050477981567, acc: 0.9609, auc: 0.9895, precision: 0.9412, recall: 0.96\n",
      "2019-05-21T14:22:16.754180, step: 433, loss: 0.15000450611114502, acc: 0.9453, auc: 0.9884, precision: 0.9636, recall: 0.9138\n",
      "2019-05-21T14:22:16.951620, step: 434, loss: 0.2172778993844986, acc: 0.9453, auc: 0.9777, precision: 0.9841, recall: 0.9118\n",
      "2019-05-21T14:22:17.153082, step: 435, loss: 0.09999939799308777, acc: 0.9531, auc: 0.9936, precision: 0.9571, recall: 0.9571\n",
      "2019-05-21T14:22:17.347593, step: 436, loss: 0.20032484829425812, acc: 0.9375, auc: 0.9809, precision: 0.9538, recall: 0.9254\n",
      "2019-05-21T14:22:17.548055, step: 437, loss: 0.1751827448606491, acc: 0.9453, auc: 0.9844, precision: 0.9623, recall: 0.9107\n",
      "2019-05-21T14:22:17.740512, step: 438, loss: 0.08153405040502548, acc: 0.9766, auc: 0.9968, precision: 0.9692, recall: 0.9844\n",
      "2019-05-21T14:22:17.937016, step: 439, loss: 0.19747033715248108, acc: 0.9453, auc: 0.9801, precision: 0.9552, recall: 0.9412\n",
      "2019-05-21T14:22:18.130468, step: 440, loss: 0.11805658042430878, acc: 0.9453, auc: 0.9917, precision: 0.9403, recall: 0.9545\n",
      "2019-05-21T14:22:18.330932, step: 441, loss: 0.1232350617647171, acc: 0.9609, auc: 0.9938, precision: 0.9851, recall: 0.9429\n",
      "2019-05-21T14:22:18.527407, step: 442, loss: 0.13059508800506592, acc: 0.9531, auc: 0.9917, precision: 0.9672, recall: 0.9365\n",
      "2019-05-21T14:22:18.724878, step: 443, loss: 0.13968750834465027, acc: 0.9531, auc: 0.9889, precision: 0.9818, recall: 0.9153\n",
      "2019-05-21T14:22:18.920384, step: 444, loss: 0.1844404935836792, acc: 0.9453, auc: 0.9802, precision: 0.9667, recall: 0.9206\n",
      "2019-05-21T14:22:19.116860, step: 445, loss: 0.15443290770053864, acc: 0.9453, auc: 0.9892, precision: 0.9296, recall: 0.9706\n",
      "2019-05-21T14:22:19.311340, step: 446, loss: 0.2026524841785431, acc: 0.9141, auc: 0.9844, precision: 0.8939, recall: 0.9365\n",
      "2019-05-21T14:22:19.503796, step: 447, loss: 0.07103661447763443, acc: 0.9766, auc: 0.9978, precision: 0.9667, recall: 0.9831\n",
      "2019-05-21T14:22:19.698307, step: 448, loss: 0.1815342754125595, acc: 0.9375, auc: 0.9887, precision: 1.0, recall: 0.8644\n",
      "2019-05-21T14:22:19.891759, step: 449, loss: 0.21192699670791626, acc: 0.9219, auc: 0.9898, precision: 1.0, recall: 0.8182\n",
      "2019-05-21T14:22:20.090257, step: 450, loss: 0.18464060127735138, acc: 0.9219, auc: 0.986, precision: 0.9464, recall: 0.8833\n",
      "2019-05-21T14:22:20.283710, step: 451, loss: 0.148795947432518, acc: 0.9453, auc: 0.9919, precision: 0.9077, recall: 0.9833\n",
      "2019-05-21T14:22:20.496173, step: 452, loss: 0.1061326116323471, acc: 0.9531, auc: 0.9953, precision: 0.9459, recall: 0.9722\n",
      "2019-05-21T14:22:20.687630, step: 453, loss: 0.30141711235046387, acc: 0.9141, auc: 0.9716, precision: 0.9, recall: 0.9403\n",
      "2019-05-21T14:22:20.878121, step: 454, loss: 0.24240611493587494, acc: 0.9375, auc: 0.9799, precision: 0.9, recall: 0.9643\n",
      "2019-05-21T14:22:21.071633, step: 455, loss: 0.1322641521692276, acc: 0.9531, auc: 0.9897, precision: 0.9649, recall: 0.9322\n",
      "2019-05-21T14:22:21.266113, step: 456, loss: 0.1764029711484909, acc: 0.9297, auc: 0.9936, precision: 1.0, recall: 0.8475\n",
      "2019-05-21T14:22:21.461561, step: 457, loss: 0.2352139949798584, acc: 0.9141, auc: 0.9794, precision: 0.9615, recall: 0.8475\n",
      "2019-05-21T14:22:21.656040, step: 458, loss: 0.17768259346485138, acc: 0.9375, auc: 0.9875, precision: 0.9828, recall: 0.8906\n",
      "2019-05-21T14:22:21.852515, step: 459, loss: 0.14471018314361572, acc: 0.9531, auc: 0.9897, precision: 0.9375, recall: 0.9677\n",
      "2019-05-21T14:22:22.047992, step: 460, loss: 0.19337669014930725, acc: 0.9297, auc: 0.9958, precision: 0.8732, recall: 1.0\n",
      "2019-05-21T14:22:22.259455, step: 461, loss: 0.15691141784191132, acc: 0.9453, auc: 0.9887, precision: 0.9333, recall: 0.9492\n",
      "2019-05-21T14:22:22.458923, step: 462, loss: 0.20234648883342743, acc: 0.9453, auc: 0.9787, precision: 0.9412, recall: 0.9231\n",
      "2019-05-21T14:22:22.652406, step: 463, loss: 0.29591432213783264, acc: 0.9219, auc: 0.9785, precision: 1.0, recall: 0.8438\n",
      "2019-05-21T14:22:22.844893, step: 464, loss: 0.2116655558347702, acc: 0.9219, auc: 0.993, precision: 1.0, recall: 0.8649\n",
      "2019-05-21T14:22:23.063277, step: 465, loss: 0.22008630633354187, acc: 0.9219, auc: 0.9812, precision: 0.9375, recall: 0.9091\n",
      "2019-05-21T14:22:23.257786, step: 466, loss: 0.2172781229019165, acc: 0.9219, auc: 0.9914, precision: 0.8696, recall: 0.9836\n",
      "2019-05-21T14:22:23.452237, step: 467, loss: 0.19042861461639404, acc: 0.9297, auc: 0.9907, precision: 0.8906, recall: 0.9661\n",
      "2019-05-21T14:22:23.648712, step: 468, loss: 0.14485350251197815, acc: 0.9688, auc: 0.9892, precision: 0.9531, recall: 0.9839\n",
      "start training model\n",
      "2019-05-21T14:22:23.856186, step: 469, loss: 0.10900050401687622, acc: 0.9688, auc: 0.9978, precision: 1.0, recall: 0.9333\n",
      "2019-05-21T14:22:24.051663, step: 470, loss: 0.2043040692806244, acc: 0.9531, auc: 0.9934, precision: 1.0, recall: 0.9032\n",
      "2019-05-21T14:22:24.245117, step: 471, loss: 0.10429015010595322, acc: 0.9531, auc: 0.9988, precision: 1.0, recall: 0.9167\n",
      "2019-05-21T14:22:24.443617, step: 472, loss: 0.06075703352689743, acc: 0.9609, auc: 0.9971, precision: 0.9697, recall: 0.9552\n",
      "2019-05-21T14:22:24.645048, step: 473, loss: 0.026036161929368973, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:22:24.845513, step: 474, loss: 0.05411454662680626, acc: 0.9922, auc: 0.9998, precision: 0.9836, recall: 1.0\n",
      "2019-05-21T14:22:25.044978, step: 475, loss: 0.2310667783021927, acc: 0.9297, auc: 0.9963, precision: 0.8831, recall: 1.0\n",
      "2019-05-21T14:22:25.239458, step: 476, loss: 0.08324425667524338, acc: 0.9688, auc: 0.9993, precision: 0.9483, recall: 0.9821\n",
      "2019-05-21T14:22:25.442914, step: 477, loss: 0.04884995520114899, acc: 0.9922, auc: 0.9987, precision: 0.9815, recall: 1.0\n",
      "2019-05-21T14:22:25.639389, step: 478, loss: 0.09679447114467621, acc: 0.9375, auc: 1.0, precision: 1.0, recall: 0.8857\n",
      "2019-05-21T14:22:25.834866, step: 479, loss: 0.14090000092983246, acc: 0.9375, auc: 0.999, precision: 1.0, recall: 0.8904\n",
      "2019-05-21T14:22:26.027384, step: 480, loss: 0.17368102073669434, acc: 0.9141, auc: 1.0, precision: 1.0, recall: 0.8197\n",
      "2019-05-21T14:22:26.222857, step: 481, loss: 0.0594780370593071, acc: 0.9766, auc: 0.9972, precision: 0.9863, recall: 0.973\n",
      "2019-05-21T14:22:26.416343, step: 482, loss: 0.09030045568943024, acc: 0.9844, auc: 1.0, precision: 0.9608, recall: 1.0\n",
      "2019-05-21T14:22:26.610823, step: 483, loss: 0.21881267428398132, acc: 0.9453, auc: 0.9978, precision: 0.9, recall: 1.0\n",
      "2019-05-21T14:22:26.819277, step: 484, loss: 0.173320934176445, acc: 0.9453, auc: 0.9946, precision: 0.9103, recall: 1.0\n",
      "2019-05-21T14:22:27.033660, step: 485, loss: 0.025007054209709167, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9667\n",
      "2019-05-21T14:22:27.228172, step: 486, loss: 0.05607720464468002, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9538\n",
      "2019-05-21T14:22:27.421654, step: 487, loss: 0.11998386681079865, acc: 0.9219, auc: 0.9998, precision: 1.0, recall: 0.8529\n",
      "2019-05-21T14:22:27.619095, step: 488, loss: 0.14727409183979034, acc: 0.9375, auc: 0.9971, precision: 1.0, recall: 0.871\n",
      "2019-05-21T14:22:27.817594, step: 489, loss: 0.04336687549948692, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9753\n",
      "2019-05-21T14:22:28.015068, step: 490, loss: 0.055062033236026764, acc: 0.9688, auc: 0.999, precision: 0.9577, recall: 0.9855\n",
      "2019-05-21T14:22:28.206556, step: 491, loss: 0.15058477222919464, acc: 0.9688, auc: 0.9985, precision: 0.9394, recall: 1.0\n",
      "2019-05-21T14:22:28.407985, step: 492, loss: 0.12097343057394028, acc: 0.9453, auc: 0.9961, precision: 0.9104, recall: 0.9839\n",
      "2019-05-21T14:22:28.631388, step: 493, loss: 0.0846794918179512, acc: 0.9609, auc: 0.9983, precision: 0.9565, recall: 0.9706\n",
      "2019-05-21T14:22:28.826894, step: 494, loss: 0.04206696152687073, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:22:29.021375, step: 495, loss: 0.0854085236787796, acc: 0.9453, auc: 1.0, precision: 1.0, recall: 0.8833\n",
      "2019-05-21T14:22:29.218846, step: 496, loss: 0.07378601282835007, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9577\n",
      "2019-05-21T14:22:29.414323, step: 497, loss: 0.036204949021339417, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9718\n",
      "2019-05-21T14:22:29.610800, step: 498, loss: 0.07281112670898438, acc: 0.9375, auc: 0.9956, precision: 0.9672, recall: 0.9077\n",
      "2019-05-21T14:22:29.803283, step: 499, loss: 0.058760061860084534, acc: 0.9844, auc: 0.9995, precision: 0.9697, recall: 1.0\n",
      "2019-05-21T14:22:30.009734, step: 500, loss: 0.017608877271413803, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-05-21T14:22:37.672212, step: 500, loss: 0.5891967484584222, acc: 0.863992307692308, auc: 0.9416358974358972, precision: 0.839053846153846, recall: 0.9019128205128205\n",
      "2019-05-21T14:22:37.867689, step: 501, loss: 0.08199064433574677, acc: 0.9688, auc: 0.9965, precision: 0.9589, recall: 0.9859\n",
      "2019-05-21T14:22:38.068185, step: 502, loss: 0.1639900803565979, acc: 0.9609, auc: 0.9908, precision: 0.9474, recall: 0.9643\n",
      "2019-05-21T14:22:38.261636, step: 503, loss: 0.02786051109433174, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-05-21T14:22:38.455118, step: 504, loss: 0.05064871907234192, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9474\n",
      "2019-05-21T14:22:38.648630, step: 505, loss: 0.03477043658494949, acc: 0.9766, auc: 0.9992, precision: 1.0, recall: 0.9423\n",
      "2019-05-21T14:22:38.845108, step: 506, loss: 0.07832475751638412, acc: 0.9688, auc: 0.9978, precision: 0.9851, recall: 0.9565\n",
      "2019-05-21T14:22:39.047534, step: 507, loss: 0.06451350450515747, acc: 0.9766, auc: 0.9978, precision: 0.9661, recall: 0.9828\n",
      "2019-05-21T14:22:39.242050, step: 508, loss: 0.06745365262031555, acc: 0.9688, auc: 0.9978, precision: 0.9836, recall: 0.9524\n",
      "2019-05-21T14:22:39.441509, step: 509, loss: 0.029965102672576904, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9672\n",
      "2019-05-21T14:22:39.634963, step: 510, loss: 0.06107235327363014, acc: 0.9766, auc: 0.9977, precision: 0.9737, recall: 0.9867\n",
      "2019-05-21T14:22:39.832464, step: 511, loss: 0.07999887317419052, acc: 0.9844, auc: 0.9963, precision: 0.9683, recall: 1.0\n",
      "2019-05-21T14:22:40.025918, step: 512, loss: 0.08675047755241394, acc: 0.9609, auc: 0.9966, precision: 0.95, recall: 0.9661\n",
      "2019-05-21T14:22:40.218404, step: 513, loss: 0.12771964073181152, acc: 0.9688, auc: 0.9927, precision: 1.0, recall: 0.9385\n",
      "2019-05-21T14:22:40.431832, step: 514, loss: 0.07844531536102295, acc: 0.9688, auc: 0.9968, precision: 0.9661, recall: 0.9661\n",
      "2019-05-21T14:22:40.634326, step: 515, loss: 0.0837683156132698, acc: 0.9688, auc: 0.9973, precision: 0.9559, recall: 0.9848\n",
      "2019-05-21T14:22:40.828800, step: 516, loss: 0.076390340924263, acc: 0.9688, auc: 0.9966, precision: 0.9531, recall: 0.9839\n",
      "2019-05-21T14:22:41.027270, step: 517, loss: 0.0734003484249115, acc: 0.9688, auc: 0.9985, precision: 0.9841, recall: 0.9538\n",
      "2019-05-21T14:22:41.224741, step: 518, loss: 0.03769379109144211, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9531\n",
      "2019-05-21T14:22:41.425205, step: 519, loss: 0.15951307117938995, acc: 0.9766, auc: 0.9851, precision: 0.9844, recall: 0.9692\n",
      "2019-05-21T14:22:41.622648, step: 520, loss: 0.032821375876665115, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9672\n",
      "2019-05-21T14:22:41.834114, step: 521, loss: 0.02732769027352333, acc: 0.9922, auc: 0.9998, precision: 0.9833, recall: 1.0\n",
      "2019-05-21T14:22:42.031555, step: 522, loss: 0.047336503863334656, acc: 0.9844, auc: 0.999, precision: 0.9851, recall: 0.9851\n",
      "2019-05-21T14:22:42.226064, step: 523, loss: 0.09035629034042358, acc: 0.9688, auc: 0.9978, precision: 0.9492, recall: 0.9825\n",
      "2019-05-21T14:22:42.418550, step: 524, loss: 0.026814835146069527, acc: 0.9922, auc: 0.9998, precision: 0.9844, recall: 1.0\n",
      "2019-05-21T14:22:42.628986, step: 525, loss: 0.050298742949962616, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9836\n",
      "2019-05-21T14:22:42.822471, step: 526, loss: 0.05403634160757065, acc: 0.9688, auc: 0.999, precision: 1.0, recall: 0.9167\n",
      "2019-05-21T14:22:43.015922, step: 527, loss: 0.0892355889081955, acc: 0.9531, auc: 0.9987, precision: 1.0, recall: 0.9231\n",
      "2019-05-21T14:22:43.208408, step: 528, loss: 0.03634122014045715, acc: 0.9844, auc: 0.9993, precision: 0.9859, recall: 0.9859\n",
      "2019-05-21T14:22:43.405879, step: 529, loss: 0.047264181077480316, acc: 0.9766, auc: 1.0, precision: 0.9595, recall: 1.0\n",
      "2019-05-21T14:22:43.604349, step: 530, loss: 0.08626006543636322, acc: 0.9688, auc: 0.9995, precision: 0.9286, recall: 1.0\n",
      "2019-05-21T14:22:43.800852, step: 531, loss: 0.07347200810909271, acc: 0.9688, auc: 0.9971, precision: 0.9846, recall: 0.9552\n",
      "2019-05-21T14:22:43.995332, step: 532, loss: 0.06084630265831947, acc: 0.9766, auc: 0.9985, precision: 0.9851, recall: 0.9706\n",
      "2019-05-21T14:22:44.186817, step: 533, loss: 0.03985327482223511, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9661\n",
      "2019-05-21T14:22:44.381271, step: 534, loss: 0.044613830745220184, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9138\n",
      "2019-05-21T14:22:44.581764, step: 535, loss: 0.033004436641931534, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:22:44.778240, step: 536, loss: 0.06292446702718735, acc: 0.9844, auc: 0.9973, precision: 0.9848, recall: 0.9848\n",
      "2019-05-21T14:22:44.971693, step: 537, loss: 0.12942124903202057, acc: 0.9766, auc: 0.9875, precision: 0.9839, recall: 0.9683\n",
      "2019-05-21T14:22:45.167170, step: 538, loss: 0.05738823115825653, acc: 0.9766, auc: 0.9978, precision: 0.9841, recall: 0.9688\n",
      "2019-05-21T14:22:45.361649, step: 539, loss: 0.03402028977870941, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9844\n",
      "2019-05-21T14:22:45.560119, step: 540, loss: 0.03436378017067909, acc: 0.9844, auc: 0.9995, precision: 0.9818, recall: 0.9818\n",
      "2019-05-21T14:22:45.757620, step: 541, loss: 0.054535530507564545, acc: 0.9609, auc: 0.9983, precision: 0.9844, recall: 0.9403\n",
      "2019-05-21T14:22:45.971020, step: 542, loss: 0.07966463267803192, acc: 0.9609, auc: 0.9966, precision: 0.9661, recall: 0.95\n",
      "2019-05-21T14:22:46.162508, step: 543, loss: 0.028760461136698723, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.971\n",
      "2019-05-21T14:22:46.364967, step: 544, loss: 0.06477083265781403, acc: 0.9844, auc: 0.999, precision: 0.9677, recall: 1.0\n",
      "2019-05-21T14:22:46.568455, step: 545, loss: 0.08731193840503693, acc: 0.9688, auc: 0.9956, precision: 0.9833, recall: 0.9516\n",
      "2019-05-21T14:22:46.762931, step: 546, loss: 0.022729646414518356, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2019-05-21T14:22:46.963396, step: 547, loss: 0.08802200853824615, acc: 0.9609, auc: 0.9968, precision: 0.9508, recall: 0.9667\n",
      "2019-05-21T14:22:47.158872, step: 548, loss: 0.0613044835627079, acc: 0.9688, auc: 0.9983, precision: 1.0, recall: 0.9273\n",
      "2019-05-21T14:22:47.380282, step: 549, loss: 0.01773461326956749, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:22:47.583739, step: 550, loss: 0.08228624612092972, acc: 0.9609, auc: 0.9971, precision: 0.9839, recall: 0.9385\n",
      "2019-05-21T14:22:47.778188, step: 551, loss: 0.02567402832210064, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9841\n",
      "2019-05-21T14:22:47.976686, step: 552, loss: 0.06570608913898468, acc: 0.9766, auc: 0.9978, precision: 0.971, recall: 0.9853\n",
      "2019-05-21T14:22:48.169142, step: 553, loss: 0.01950351893901825, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:22:48.365645, step: 554, loss: 0.09629285335540771, acc: 0.9609, auc: 0.9958, precision: 0.9577, recall: 0.9714\n",
      "2019-05-21T14:22:48.565083, step: 555, loss: 0.024720115587115288, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:22:48.768539, step: 556, loss: 0.050250306725502014, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9492\n",
      "2019-05-21T14:22:48.964017, step: 557, loss: 0.055998362600803375, acc: 0.9531, auc: 0.9988, precision: 0.9804, recall: 0.9091\n",
      "2019-05-21T14:22:49.163514, step: 558, loss: 0.04067710041999817, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9848\n",
      "2019-05-21T14:22:49.365974, step: 559, loss: 0.07600799202919006, acc: 0.9766, auc: 0.9971, precision: 0.9836, recall: 0.9677\n",
      "2019-05-21T14:22:49.566406, step: 560, loss: 0.04901082441210747, acc: 0.9844, auc: 0.999, precision: 0.9818, recall: 0.9818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:22:49.760915, step: 561, loss: 0.015103764832019806, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:22:49.960352, step: 562, loss: 0.06726492196321487, acc: 0.9844, auc: 0.9968, precision: 0.9841, recall: 0.9841\n",
      "2019-05-21T14:22:50.155861, step: 563, loss: 0.0332767628133297, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9444\n",
      "2019-05-21T14:22:50.356295, step: 564, loss: 0.10883253067731857, acc: 0.9688, auc: 0.9944, precision: 0.9825, recall: 0.9492\n",
      "2019-05-21T14:22:50.557788, step: 565, loss: 0.05533318594098091, acc: 0.9766, auc: 0.998, precision: 0.9863, recall: 0.973\n",
      "2019-05-21T14:22:50.753232, step: 566, loss: 0.05400428548455238, acc: 0.9844, auc: 0.9978, precision: 0.9833, recall: 0.9833\n",
      "2019-05-21T14:22:50.948718, step: 567, loss: 0.0912766233086586, acc: 0.9766, auc: 0.9961, precision: 0.9677, recall: 0.9836\n",
      "2019-05-21T14:22:51.143219, step: 568, loss: 0.06280166655778885, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9524\n",
      "2019-05-21T14:22:51.357616, step: 569, loss: 0.046289559453725815, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-05-21T14:22:51.556114, step: 570, loss: 0.07102988660335541, acc: 0.9609, auc: 0.9978, precision: 0.9844, recall: 0.9403\n",
      "2019-05-21T14:22:51.752560, step: 571, loss: 0.041517045348882675, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9848\n",
      "2019-05-21T14:22:51.946072, step: 572, loss: 0.060637377202510834, acc: 0.9688, auc: 0.998, precision: 0.9643, recall: 0.9643\n",
      "2019-05-21T14:22:52.143514, step: 573, loss: 0.06135538965463638, acc: 0.9609, auc: 0.9988, precision: 1.0, recall: 0.9206\n",
      "2019-05-21T14:22:52.335031, step: 574, loss: 0.039705321192741394, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9559\n",
      "2019-05-21T14:22:52.531505, step: 575, loss: 0.014170296490192413, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:22:52.734933, step: 576, loss: 0.04873031750321388, acc: 0.9766, auc: 0.9987, precision: 0.9863, recall: 0.973\n",
      "2019-05-21T14:22:52.929413, step: 577, loss: 0.12112168967723846, acc: 0.9609, auc: 0.9946, precision: 0.9375, recall: 0.9836\n",
      "2019-05-21T14:22:53.122897, step: 578, loss: 0.04432307183742523, acc: 0.9844, auc: 1.0, precision: 0.9714, recall: 1.0\n",
      "2019-05-21T14:22:53.315381, step: 579, loss: 0.03274070471525192, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9839\n",
      "2019-05-21T14:22:53.509861, step: 580, loss: 0.11934661865234375, acc: 0.9453, auc: 0.9939, precision: 0.9538, recall: 0.9394\n",
      "2019-05-21T14:22:53.700351, step: 581, loss: 0.03363482281565666, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9688\n",
      "2019-05-21T14:22:53.895857, step: 582, loss: 0.02982846461236477, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9861\n",
      "2019-05-21T14:22:54.089384, step: 583, loss: 0.07761098444461823, acc: 0.9766, auc: 0.9973, precision: 0.9818, recall: 0.9643\n",
      "2019-05-21T14:22:54.284858, step: 584, loss: 0.08648183941841125, acc: 0.9766, auc: 0.9976, precision: 0.9836, recall: 0.9677\n",
      "2019-05-21T14:22:54.481333, step: 585, loss: 0.05710427463054657, acc: 0.9688, auc: 0.998, precision: 0.9815, recall: 0.9464\n",
      "2019-05-21T14:22:54.678806, step: 586, loss: 0.04727542772889137, acc: 0.9688, auc: 0.9988, precision: 0.9841, recall: 0.9538\n",
      "2019-05-21T14:22:54.880267, step: 587, loss: 0.06358066201210022, acc: 0.9688, auc: 0.9983, precision: 0.9722, recall: 0.9722\n",
      "2019-05-21T14:22:55.089677, step: 588, loss: 0.18987563252449036, acc: 0.9531, auc: 0.9888, precision: 0.9385, recall: 0.9683\n",
      "2019-05-21T14:22:55.291170, step: 589, loss: 0.03561476618051529, acc: 0.9766, auc: 0.9993, precision: 0.9833, recall: 0.9672\n",
      "2019-05-21T14:22:55.486647, step: 590, loss: 0.08671681582927704, acc: 0.9766, auc: 0.9978, precision: 0.9672, recall: 0.9833\n",
      "2019-05-21T14:22:55.683119, step: 591, loss: 0.06293898820877075, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9531\n",
      "2019-05-21T14:22:55.882585, step: 592, loss: 0.06109020859003067, acc: 0.9609, auc: 0.9975, precision: 0.9861, recall: 0.9467\n",
      "2019-05-21T14:22:56.078066, step: 593, loss: 0.05268217623233795, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9492\n",
      "2019-05-21T14:22:56.275535, step: 594, loss: 0.035523660480976105, acc: 0.9922, auc: 0.9998, precision: 0.9853, recall: 1.0\n",
      "2019-05-21T14:22:56.473008, step: 595, loss: 0.09802968800067902, acc: 0.9609, auc: 0.9949, precision: 0.9508, recall: 0.9667\n",
      "2019-05-21T14:22:56.673468, step: 596, loss: 0.08492635190486908, acc: 0.9766, auc: 0.9946, precision: 0.9508, recall: 1.0\n",
      "2019-05-21T14:22:56.872910, step: 597, loss: 0.04531814903020859, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9844\n",
      "2019-05-21T14:22:57.069415, step: 598, loss: 0.057817935943603516, acc: 0.9766, auc: 0.9985, precision: 0.9815, recall: 0.9636\n",
      "2019-05-21T14:22:57.269879, step: 599, loss: 0.045189470052719116, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9672\n",
      "2019-05-21T14:22:57.480317, step: 600, loss: 0.049436844885349274, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9552\n",
      "\n",
      "Evaluation:\n",
      "2019-05-21T14:23:05.103930, step: 600, loss: 0.5454066338447424, acc: 0.8591769230769233, auc: 0.9425564102564099, precision: 0.9060769230769233, recall: 0.8050871794871797\n",
      "2019-05-21T14:23:05.296432, step: 601, loss: 0.04602990671992302, acc: 0.9766, auc: 0.999, precision: 0.9821, recall: 0.9649\n",
      "2019-05-21T14:23:05.488898, step: 602, loss: 0.07334762811660767, acc: 0.9766, auc: 0.9953, precision: 0.9714, recall: 0.9855\n",
      "2019-05-21T14:23:05.689333, step: 603, loss: 0.09313324838876724, acc: 0.9688, auc: 0.9978, precision: 0.9492, recall: 0.9825\n",
      "2019-05-21T14:23:05.889831, step: 604, loss: 0.08277768641710281, acc: 0.9688, auc: 0.9963, precision: 0.9831, recall: 0.9508\n",
      "2019-05-21T14:23:06.085305, step: 605, loss: 0.05530340597033501, acc: 0.9688, auc: 0.9973, precision: 0.9661, recall: 0.9661\n",
      "2019-05-21T14:23:06.276762, step: 606, loss: 0.06158324331045151, acc: 0.9766, auc: 0.9983, precision: 1.0, recall: 0.9524\n",
      "2019-05-21T14:23:06.466286, step: 607, loss: 0.07214666903018951, acc: 0.9766, auc: 0.9995, precision: 0.9508, recall: 1.0\n",
      "2019-05-21T14:23:06.661761, step: 608, loss: 0.08797232061624527, acc: 0.9688, auc: 0.9946, precision: 0.9571, recall: 0.9853\n",
      "2019-05-21T14:23:06.855246, step: 609, loss: 0.03893483057618141, acc: 0.9766, auc: 0.9988, precision: 0.9821, recall: 0.9649\n",
      "2019-05-21T14:23:07.054713, step: 610, loss: 0.11495465040206909, acc: 0.9453, auc: 0.9949, precision: 0.9833, recall: 0.9077\n",
      "2019-05-21T14:23:07.247206, step: 611, loss: 0.06252949684858322, acc: 0.9688, auc: 0.9978, precision: 0.9836, recall: 0.9524\n",
      "2019-05-21T14:23:07.440679, step: 612, loss: 0.046800144016742706, acc: 0.9844, auc: 0.9983, precision: 1.0, recall: 0.9714\n",
      "2019-05-21T14:23:07.636156, step: 613, loss: 0.07461043447256088, acc: 0.9688, auc: 0.9968, precision: 0.9836, recall: 0.9524\n",
      "2019-05-21T14:23:07.845596, step: 614, loss: 0.05850435793399811, acc: 0.9844, auc: 0.998, precision: 0.9855, recall: 0.9855\n",
      "2019-05-21T14:23:08.047028, step: 615, loss: 0.08635701984167099, acc: 0.9688, auc: 0.9995, precision: 0.9429, recall: 1.0\n",
      "2019-05-21T14:23:08.245528, step: 616, loss: 0.026752019301056862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:23:08.442998, step: 617, loss: 0.06980405002832413, acc: 0.9688, auc: 0.9968, precision: 0.9821, recall: 0.9483\n",
      "2019-05-21T14:23:08.643434, step: 618, loss: 0.07484915852546692, acc: 0.9688, auc: 0.9975, precision: 1.0, recall: 0.942\n",
      "2019-05-21T14:23:08.834952, step: 619, loss: 0.07354573160409927, acc: 0.9688, auc: 0.9978, precision: 0.9688, recall: 0.9688\n",
      "2019-05-21T14:23:09.028434, step: 620, loss: 0.03917642682790756, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9508\n",
      "2019-05-21T14:23:09.221915, step: 621, loss: 0.049303993582725525, acc: 0.9766, auc: 0.9985, precision: 0.9846, recall: 0.9697\n",
      "2019-05-21T14:23:09.419388, step: 622, loss: 0.06350181996822357, acc: 0.9922, auc: 0.9976, precision: 0.9841, recall: 1.0\n",
      "2019-05-21T14:23:09.615862, step: 623, loss: 0.08056826889514923, acc: 0.9688, auc: 0.9993, precision: 0.9429, recall: 1.0\n",
      "2019-05-21T14:23:09.809344, step: 624, loss: 0.08678517490625381, acc: 0.9609, auc: 0.9963, precision: 0.9565, recall: 0.9706\n",
      "start training model\n",
      "2019-05-21T14:23:10.022776, step: 625, loss: 0.017494887113571167, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T14:23:10.229224, step: 626, loss: 0.054266832768917084, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9667\n",
      "2019-05-21T14:23:10.423702, step: 627, loss: 0.032248105853796005, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-05-21T14:23:10.621176, step: 628, loss: 0.029827618971467018, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9474\n",
      "2019-05-21T14:23:10.815653, step: 629, loss: 0.02614426240324974, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-05-21T14:23:11.014129, step: 630, loss: 0.0415182039141655, acc: 0.9766, auc: 0.9993, precision: 0.9726, recall: 0.9861\n",
      "2019-05-21T14:23:11.207607, step: 631, loss: 0.033303432166576385, acc: 0.9922, auc: 0.9998, precision: 0.9844, recall: 1.0\n",
      "2019-05-21T14:23:11.417017, step: 632, loss: 0.061759453266859055, acc: 0.9844, auc: 0.9975, precision: 0.9677, recall: 1.0\n",
      "2019-05-21T14:23:11.616512, step: 633, loss: 0.02786361426115036, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9828\n",
      "2019-05-21T14:23:11.821934, step: 634, loss: 0.01988457329571247, acc: 0.9922, auc: 0.9998, precision: 0.9831, recall: 1.0\n",
      "2019-05-21T14:23:12.020432, step: 635, loss: 0.03128599748015404, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9583\n",
      "2019-05-21T14:23:12.218901, step: 636, loss: 0.04248393699526787, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9385\n",
      "2019-05-21T14:23:12.413353, step: 637, loss: 0.009032054804265499, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-05-21T14:23:12.607865, step: 638, loss: 0.004711668938398361, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:23:12.805333, step: 639, loss: 0.017388945445418358, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:23:13.002806, step: 640, loss: 0.05336558073759079, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2019-05-21T14:23:13.198254, step: 641, loss: 0.014630276709794998, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2019-05-21T14:23:13.397756, step: 642, loss: 0.030619410797953606, acc: 0.9922, auc: 0.9992, precision: 0.9815, recall: 1.0\n",
      "2019-05-21T14:23:13.593233, step: 643, loss: 0.02168898656964302, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:23:13.788705, step: 644, loss: 0.008938513696193695, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:23:13.986176, step: 645, loss: 0.015980159863829613, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9861\n",
      "2019-05-21T14:23:14.180664, step: 646, loss: 0.02122080884873867, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.98\n",
      "2019-05-21T14:23:14.377131, step: 647, loss: 0.00412007886916399, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-05-21T14:23:14.573604, step: 648, loss: 0.0323403924703598, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9672\n",
      "2019-05-21T14:23:14.774070, step: 649, loss: 0.020361801609396935, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9672\n",
      "2019-05-21T14:23:14.970543, step: 650, loss: 0.04968186467885971, acc: 0.9844, auc: 1.0, precision: 0.9692, recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "embeddedPosition = fixedPositionEmbedding(config.batchSize, config.sequenceLength)\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        transformer = Transformer(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(transformer.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", transformer.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Transformer/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: config.model.dropoutKeepProb,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: 1.0,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Transformer/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(transformer.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(transformer.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(transformer.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
